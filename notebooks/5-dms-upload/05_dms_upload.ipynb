{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. DMS Upload Demo\n",
        "\n",
        "This notebook demonstrates how to upload a document into the local DMS mock using our `src/dms` service layer.\n",
        "\n",
        "> **ðŸ“– For background**: We attach to the services started via `compose.yml` (PostgreSQL and Azurite) and perform a simple endâ€‘toâ€‘end upload and verification.\n",
        "\n",
        "## What you will do\n",
        "- **Initialize environment**: Resolve project root, import libraries, and create clients\n",
        "- **Ensure schema**: Apply minimal tables if missing\n",
        "- **Upload**: Push a sample PDF to blob storage via the DMS service\n",
        "- **Verify**: Read back metadata and download the file\n",
        "\n",
        "**Estimated time:** 2â€“3 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prerequisites\n",
        "\n",
        "- Docker Desktop running\n",
        "- `uv` installed with project dependencies (`uv sync`)\n",
        "- Services started with `docker compose up -d` from the project root\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment configuration\n",
        "\n",
        "### 2.1 Imports and project root detection\n",
        "\n",
        "We use the same root detection approach as Notebook 01 to ensure imports from `src/*` work inside this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/markuskuehnle/Documents/projects/credit-ocr-system\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import psycopg2\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "# Project root detection (same as Notebook 01)\n",
        "current_directory = Path.cwd()\n",
        "if current_directory.name == \"5-dms-upload\" and current_directory.parent.name == \"notebooks\":\n",
        "    project_root_directory = current_directory.parent.parent\n",
        "elif (current_directory / \"compose.yml\").exists():\n",
        "    project_root_directory = current_directory\n",
        "else:\n",
        "    project_root_directory = None\n",
        "\n",
        "if not project_root_directory or not (project_root_directory / \"compose.yml\").exists():\n",
        "    raise RuntimeError(\"Cannot find project root (compose.yml not found). Run notebook from repo or notebooks folder.\")\n",
        "\n",
        "if str(project_root_directory) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root_directory))\n",
        "\n",
        "# Adapters and service (ensure latest code is loaded in this kernel)\n",
        "import importlib\n",
        "import src.dms.adapters as dms_adapters\n",
        "importlib.reload(dms_adapters)\n",
        "from src.dms.adapters import AzureBlobStorageClient, PostgresMetadataRepository\n",
        "from src.dms.service import DmsService\n",
        "\n",
        "print(f\"Project root: {project_root_directory}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Service clients\n",
        "\n",
        "We attach to the running Postgres and Azurite services from `compose.yml` and construct the DMS service with adapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment ready: connected to Postgres and Azurite\n"
          ]
        }
      ],
      "source": [
        "# Config for compose-based services\n",
        "POSTGRES_HOST: str = \"localhost\"\n",
        "POSTGRES_PORT: int = 5432\n",
        "POSTGRES_DBNAME: str = \"dms_meta\"\n",
        "POSTGRES_USER: str = \"dms\"\n",
        "POSTGRES_PASSWORD: str = \"dms\"\n",
        "\n",
        "AZURITE_BLOB_PORT: int = 10000\n",
        "AZURITE_ACCOUNT_NAME: str = \"devstoreaccount1\"\n",
        "AZURITE_ACCOUNT_KEY: str = (\n",
        "    \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/\"\n",
        "    \"K1SZFPTOtr/KBHBeksoGMGw==\"\n",
        ")\n",
        "CONTAINER_NAME: str = \"documents\"\n",
        "\n",
        "# Optionally reuse an existing connection string if set\n",
        "existing_conn_str: str | None = os.environ.get(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
        "if existing_conn_str:\n",
        "    connection_string: str = existing_conn_str\n",
        "else:\n",
        "    connection_string = (\n",
        "        \"DefaultEndpointsProtocol=http;\"\n",
        "        f\"AccountName={AZURITE_ACCOUNT_NAME};\"\n",
        "        f\"AccountKey={AZURITE_ACCOUNT_KEY};\"\n",
        "        f\"BlobEndpoint=http://localhost:{AZURITE_BLOB_PORT}/devstoreaccount1;\"\n",
        "    )\n",
        "\n",
        "# Initialize clients\n",
        "blob_service_client: BlobServiceClient = BlobServiceClient.from_connection_string(connection_string)\n",
        "container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "pg_conn = psycopg2.connect(\n",
        "    host=POSTGRES_HOST,\n",
        "    port=POSTGRES_PORT,\n",
        "    database=POSTGRES_DBNAME,\n",
        "    user=POSTGRES_USER,\n",
        "    password=POSTGRES_PASSWORD,\n",
        ")\n",
        "\n",
        "storage_client = AzureBlobStorageClient(blob_service_client)\n",
        "metadata_repo = PostgresMetadataRepository(pg_conn)\n",
        "\n",
        "dms_service = DmsService(storage_client=storage_client, metadata_repository=metadata_repo)\n",
        "\n",
        "print(\"Environment ready: connected to Postgres and Azurite\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Database schema\n",
        "\n",
        "We'll apply the minimal schema so the `documents` table exists. This is idempotent and safe to re-run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema ensured (documents, ocr_results)\n"
          ]
        }
      ],
      "source": [
        "schema_path = project_root_directory / \"database\" / \"schemas\" / \"schema.sql\"\n",
        "if not schema_path.exists():\n",
        "    raise RuntimeError(f\"Schema file not found at {schema_path}\")\n",
        "\n",
        "with pg_conn.cursor() as cur:\n",
        "    with open(schema_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        cur.execute(f.read())\n",
        "\n",
        "print(\"Schema ensured (documents, ocr_results)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload workflow\n",
        "\n",
        "We'll upload the sample file, then fetch metadata and download the blob to confirm it is stored correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploaded: 7a385202-712d-461b-abbf-cfeb4de4b809\n",
            "Metadata keys: ['blob_path', 'document_type', 'file_size', 'hash_sha256', 'id', 'linked_entity', 'linked_entity_id', 'mime_type', 'source_filename', 'textextraction_status', 'uploaded_at']\n",
            "Downloaded bytes: 147568\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Resolve sample file\n",
        "sample_pdf_path = project_root_directory / \"data\" / \"loan_application.pdf\"\n",
        "assert sample_pdf_path.exists(), \"Sample PDF not found\"\n",
        "\n",
        "DOCUMENT_TYPE: str = \"loan-application\"\n",
        "\n",
        "# Upload\n",
        "document_id: str = dms_service.upload_document(\n",
        "    file_path=sample_pdf_path,\n",
        "    document_type=DOCUMENT_TYPE,\n",
        "    source_filename=sample_pdf_path.name,\n",
        "    linked_entity=\"CREDIT_APPLICATION\",\n",
        "    linked_entity_id=\"CA-\" + datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n",
        ")\n",
        "\n",
        "print(\"Uploaded:\", document_id)\n",
        "\n",
        "# Metadata\n",
        "metadata = dms_service.get_document(document_id)\n",
        "print(\"Metadata keys:\", sorted(list(metadata.keys())) if metadata else None)\n",
        "\n",
        "# Download bytes\n",
        "downloaded = dms_service.download_document(document_id)\n",
        "print(\"Downloaded bytes:\", len(downloaded) if downloaded else None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Results summary\n",
        "\n",
        "A compact summary of what we uploaded and what is stored.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'document_id': '7a385202-712d-461b-abbf-cfeb4de4b809',\n",
              " 'blob_path': 'raw/loan-application/7a385202-712d-461b-abbf-cfeb4de4b809.pdf',\n",
              " 'downloaded_bytes': 147568}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary = {\n",
        "    \"document_id\": document_id,\n",
        "    \"blob_path\": metadata.get(\"blob_path\") if metadata else None,\n",
        "    \"downloaded_bytes\": len(downloaded) if downloaded else 0,\n",
        "}\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Cleanup\n",
        "\n",
        "Close database connections. The uploaded blob remains for inspection unless removed manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closed PostgreSQL connection\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    pg_conn.close()\n",
        "    print(\"Closed PostgreSQL connection\")\n",
        "except Exception:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "You have successfully uploaded a document to the local DMS mock.\n",
        "\n",
        "**What we did:**\n",
        "- Connected to PostgreSQL (`dms_meta`) and Azurite (Blob Storage)\n",
        "- Ensured the minimal database schema is present\n",
        "- Uploaded `loan_application.pdf` through `DmsService`\n",
        "- Verified metadata and downloaded the stored blob\n",
        "\n",
        "**You're ready for:**\n",
        "- Creating additional document uploads\n",
        "- Linking uploads to downstream OCR and LLM pipelines\n",
        "- Extending the schema for richer document metadata and processing statuses"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
