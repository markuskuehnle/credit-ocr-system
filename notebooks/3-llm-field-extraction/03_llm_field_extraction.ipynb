{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1707f9",
   "metadata": {},
   "source": [
    "## 2. Text Cleaning and Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde44a4",
   "metadata": {},
   "source": [
    "> Use notebook 01_setup.ipynb to setup the environment and start the docker services before running the next sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e216a",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f573118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import aiohttp\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional\n",
    "import json\n",
    "import re\n",
    "from dataclasses import asdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e573f7",
   "metadata": {},
   "source": [
    "We're creating a dataclass:\n",
    "\n",
    "**`DocumentTypeConfig`**: Represents the configuration for a specific document type (like \"credit_request\")\n",
    "   - Contains field definitions, validation rules, and descriptions\n",
    "   - Makes it easy to access structured configuration data\n",
    "   - Provides type safety when working with document configurations\n",
    "\n",
    "This automatically gives us:\n",
    "- A constructor that takes these 4 parameters\n",
    "- A nice string representation when we print the object\n",
    "- Type hints for better IDE support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c08037ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentTypeConfig:\n",
    "    name: str\n",
    "    expected_fields: List[str]\n",
    "    field_descriptions: Dict[str, str]\n",
    "    validation_rules: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d43a08",
   "metadata": {},
   "source": [
    "We're using the following function to load the document type configuration from a .conf file in the config folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38755acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_config(config_path: str) -> Dict[str, DocumentTypeConfig]:\n",
    "    \"\"\"Load document configuration from JSON file.\"\"\"\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    document_types = {}\n",
    "    for doc_type, doc_config in config_data.items():\n",
    "        document_types[doc_type] = DocumentTypeConfig(\n",
    "            name=doc_config['name'],\n",
    "            expected_fields=doc_config['expected_fields'],\n",
    "            field_descriptions=doc_config['field_descriptions'],\n",
    "            validation_rules=doc_config['validation_rules'],\n",
    "        )\n",
    "\n",
    "    return document_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21677078",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_config = load_document_config(\"../../config/document_types.conf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc5be1",
   "metadata": {},
   "source": [
    "## 2. LLM Client Architecture\n",
    "\n",
    "We're creating an abstract `LLMClient` class and concrete `OllamaClient` implementation for:\n",
    "\n",
    "**Benefits:**\n",
    "- **Abstraction**: Common interface for any LLM service\n",
    "- **Flexibility**: Easy to swap between providers (Ollama, OpenAI, etc.)\n",
    "- **Testability**: Can create mock clients for testing\n",
    "- **Consistency**: All LLM clients use the same interface\n",
    "\n",
    "**Design Pattern:**\n",
    "- `LLMClient` (abstract): Defines the contract with `generate()` method\n",
    "- `OllamaClient` (concrete): Implements actual API calls to Ollama\n",
    "\n",
    "This allows us to easily add support for other LLM providers in the future without changing our core field extraction logic.\n",
    "\n",
    "We're using `@dataclass` for `GenerativeLlm` because:\n",
    "\n",
    "**Simple Data Container:**\n",
    "- Just stores configuration (URL and model name)\n",
    "- No complex logic needed\n",
    "- Perfect use case for dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c1ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATIVE_MODEL_URL = \"http://127.0.0.1:11435\"\n",
    "MODEL_NAME = \"llama3.1:8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71e1df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerativeLlm:\n",
    "    url: str\n",
    "    model_name: str\n",
    "    \n",
    "\n",
    "class LLMClient(ABC):\n",
    "    \"\"\"Abstract base class for LLM clients.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a response from the LLM.\"\"\"\n",
    "        pass\n",
    "    \n",
    "\n",
    "class OllamaClient(LLMClient):\n",
    "    \"\"\"Client for Ollama LLM service.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, model_name: str):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    async def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a response from Ollama.\"\"\"\n",
    "        timeout = aiohttp.ClientTimeout(total=120)  # 2 minutes timeout\n",
    "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "            try:\n",
    "                async with session.post(\n",
    "                    f\"{self.base_url}/api/generate\",\n",
    "                    json={\n",
    "                        \"model\": self.model_name,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"stream\": False\n",
    "                    }\n",
    "                ) as response:\n",
    "                    if response.status != 200:\n",
    "                        error_text = await response.text()\n",
    "                        raise Exception(f\"Ollama API error: {error_text}\")\n",
    "                    \n",
    "                    result = await response.json()\n",
    "                    return result.get(\"response\", \"\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(\"Error calling Ollama API\")\n",
    "                raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01c8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_llm = GenerativeLlm(\n",
    "                url=GENERATIVE_MODEL_URL,\n",
    "                model_name=MODEL_NAME,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609cfa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = OllamaClient(\n",
    "    base_url=generative_llm.url,\n",
    "    model_name=generative_llm.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a439d",
   "metadata": {},
   "source": [
    "**Core Functions for LLM Field Extraction**\n",
    "\n",
    "These functions handle the complete pipeline from OCR data to structured field extraction:\n",
    "\n",
    "- **`clean_value()`**: Converts and validates field values based on type\n",
    "- **`extract_fields_with_llm()`**: Main async function that uses LLM to extract fields\n",
    "- **`create_extraction_prompt()`**: Generates prompts for the LLM\n",
    "- **`validate_field()`** & **`validate_extracted_fields()`**: Validate extracted data against business rules\n",
    "- **`extract_json_from_response()`**: Parses LLM responses safely\n",
    "\n",
    "The pipeline: OCR data → LLM extraction → Field validation → Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e4daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_value(value: str, field_type: str) -> Any:\n",
    "    \"\"\"Clean and convert value based on field type.\"\"\"\n",
    "    if not value:\n",
    "        return None\n",
    "\n",
    "    if field_type == \"string\":\n",
    "        return value.strip()\n",
    "    \n",
    "    elif field_type == \"date\":\n",
    "        # Ensure date format DD.MM.YYYY\n",
    "        if re.match(r\"^\\d{2}\\.\\d{2}\\.\\d{4}$\", value):\n",
    "            return value\n",
    "        return None\n",
    "    \n",
    "    elif field_type == \"currency\":\n",
    "        # Remove currency symbols, spaces, and convert comma to dot\n",
    "        cleaned = value.replace(\"€\", \"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "        # Remove any non-numeric characters except decimal point\n",
    "        cleaned = ''.join(c for c in cleaned if c.isdigit() or c == '.')\n",
    "        return float(cleaned) if cleaned else None\n",
    "    \n",
    "    elif field_type == \"area\":\n",
    "        # Remove unit and spaces\n",
    "        cleaned = value.replace(\"m²\", \"\").replace(\" \", \"\")\n",
    "        return float(cleaned) if cleaned else None\n",
    "    \n",
    "    elif field_type == \"number\":\n",
    "        # Remove any non-numeric characters\n",
    "        cleaned = ''.join(c for c in value if c.isdigit())\n",
    "        return int(cleaned) if cleaned else None\n",
    "    \n",
    "    elif field_type == \"boolean\":\n",
    "        return \"[x]\" in value.lower()\n",
    "    \n",
    "    return value\n",
    "\n",
    "def extract_fields_with_llm(ocr_lines: List[Dict[str, Any]], document_type: str = \"credit_request\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract fields from OCR lines using configuration-based rules.\n",
    "    Returns a dictionary of field names to their values.\n",
    "    \"\"\"\n",
    "    # Load document configuration\n",
    "    config_path = Path(\"config/document_types.conf\")\n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "    \n",
    "    config = load_document_config(config_path)\n",
    "    if document_type not in config:\n",
    "        raise ValueError(f\"Unknown document type: {document_type}\")\n",
    "    \n",
    "    # Extract fields from OCR lines\n",
    "    extracted_fields = {}\n",
    "    field_config = config[f\"{document_type}.fields\"]\n",
    "    \n",
    "    # Map OCR lines to fields\n",
    "    for line in ocr_lines:\n",
    "        if line[\"type\"] != \"line\":\n",
    "            continue\n",
    "\n",
    "        text = line[\"text\"].strip()\n",
    "        confidence = line.get(\"confidence\", 0.5)\n",
    "\n",
    "        # Check each field's label in the configuration\n",
    "        for field_name, field_rules in field_config.items():\n",
    "            label = field_rules.get(\"label\")\n",
    "            if label and label in text:\n",
    "                # Extract value by removing the label\n",
    "                value = text.replace(label, \"\").strip()\n",
    "                # Clean and convert value based on field type\n",
    "                field_type = field_rules.get(\"type\", \"string\")\n",
    "                cleaned_value = clean_value(value, field_type)\n",
    "                if cleaned_value is not None:\n",
    "                    extracted_fields[field_name] = cleaned_value\n",
    "                break\n",
    "\n",
    "    return extracted_fields\n",
    "\n",
    "def extract_json_from_response(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"Extract JSON from LLM response, handling potential text prefixes and comments.\"\"\"\n",
    "    try:\n",
    "        # Find JSON between code blocks if present\n",
    "        if \"```\" in response:\n",
    "            # Find the first code block\n",
    "            start = response.find(\"```\")\n",
    "            if start != -1:\n",
    "                # Skip the opening ```\n",
    "                start = response.find(\"\\n\", start) + 1\n",
    "                # Find the closing ```\n",
    "                end = response.find(\"```\", start)\n",
    "                if end != -1:\n",
    "                    response = response[start:end].strip()\n",
    "        \n",
    "        # Remove any comments\n",
    "        lines = []\n",
    "        for line in response.split('\\n'):\n",
    "            if '//' in line:\n",
    "                line = line[:line.find('//')]\n",
    "            lines.append(line)\n",
    "        response = '\\n'.join(lines)\n",
    "        \n",
    "        # Try to parse the JSON\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON in response: {e}\")\n",
    "\n",
    "def create_extraction_prompt(ocr_lines: List[Dict[str, Any]], config: DocumentTypeConfig) -> str:\n",
    "    \"\"\"Create a prompt for field extraction.\"\"\"\n",
    "    # Get field descriptions (attribute or dict)\n",
    "    field_descs = (\n",
    "        config.field_descriptions\n",
    "        if hasattr(config, \"field_descriptions\")\n",
    "        else config.get(\"field_descriptions\", {})\n",
    "    )\n",
    "\n",
    "    # Format field descriptions: \"<db_key>: <human label>\"\n",
    "    field_descriptions = [f\"- {field}: {desc}\" for field, desc in field_descs.items()]\n",
    "\n",
    "    formatted_lines = []\n",
    "    for line in ocr_lines:\n",
    "        if line[\"type\"] == \"label_value\":\n",
    "            formatted_lines.append(f\"{line['label']}: {line['value']}\")\n",
    "        elif line[\"type\"] in (\"text_line\", \"line\"):\n",
    "            formatted_lines.append(line[\"text\"])\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"Extract the following fields from the document content below. Return a valid JSON object with the extracted fields.\n",
    "\n",
    "Field Descriptions:\n",
    "{chr(10).join(field_descriptions)}\n",
    "\n",
    "Document Content:\n",
    "{chr(10).join(formatted_lines)}\n",
    "\n",
    "Instructions:\n",
    "1. Return a valid JSON object with the extracted fields\n",
    "2. Use the exact field names from the mappings above\n",
    "3. Include only fields that are present in the document\n",
    "4. For fields with units (e.g., years, currency), include the unit in the value\n",
    "5. For boolean fields, return true/false\n",
    "6. For dates, use the format DD.MM.YYYY\n",
    "7. For numbers, include any units or currency symbols\n",
    "\n",
    "Example response format:\n",
    "{{\n",
    "    \"extracted_fields\": {{\n",
    "        \"company_name\": \"DemoTech GmbH\",\n",
    "        \"legal_form\": \"GmbH\",\n",
    "        \"founding_date\": \"01.01.2020\",\n",
    "        \"business_address\": \"Musterstraße 123, 12345 Berlin\",\n",
    "        \"purchase_price\": \"€500.000\",\n",
    "        \"term\": \"20 Years\",\n",
    "        \"interest_rate\": \"3,5%\"\n",
    "    }},\n",
    "    \"missing_fields\": [\"website\", \"vat_id\"],\n",
    "    \"validation_results\": {{\n",
    "        \"company_name\": {{\"valid\": true}},\n",
    "        \"legal_form\": {{\"valid\": true}},\n",
    "        \"founding_date\": {{\"valid\": true}}\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Please extract the fields from the document content above and return a JSON object in this format.\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def validate_field(value: Any, rules: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Validate a field value against validation rules.\"\"\"\n",
    "    validation_result = {\n",
    "        \"is_valid\": True,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    if not isinstance(value, dict) or \"value\" not in value:\n",
    "        validation_result[\"is_valid\"] = False\n",
    "        validation_result[\"errors\"].append(\"Invalid field format\")\n",
    "        return validation_result\n",
    "    \n",
    "    field_value = value[\"value\"]\n",
    "    \n",
    "    # Type validation\n",
    "    if \"type\" in rules:\n",
    "        expected_type = rules[\"type\"]\n",
    "        if expected_type == \"number\":\n",
    "            try:\n",
    "                # Handle German number format (1.234,56)\n",
    "                if isinstance(field_value, str):\n",
    "                    field_value = field_value.replace(\".\", \"\").replace(\",\", \".\")\n",
    "                float(field_value)\n",
    "            except (ValueError, TypeError):\n",
    "                validation_result[\"is_valid\"] = False\n",
    "                validation_result[\"errors\"].append(f\"Value must be a number\")\n",
    "        elif expected_type == \"boolean\":\n",
    "            if str(field_value).lower() not in [\"true\", \"false\"]:\n",
    "                validation_result[\"is_valid\"] = False\n",
    "                validation_result[\"errors\"].append(f\"Value must be a boolean\")\n",
    "        elif expected_type == \"date\":\n",
    "            # Skip number validation for dates\n",
    "            pass\n",
    "    \n",
    "    # Range validation (only for numbers)\n",
    "    if \"min\" in rules and \"type\" in rules and rules[\"type\"] == \"number\":\n",
    "        try:\n",
    "            if isinstance(field_value, str):\n",
    "                field_value = field_value.replace(\".\", \"\").replace(\",\", \".\")\n",
    "            if float(field_value) < rules[\"min\"]:\n",
    "                validation_result[\"is_valid\"] = False\n",
    "                validation_result[\"errors\"].append(f\"Value must be at least {rules['min']}\")\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    if \"max\" in rules and \"type\" in rules and rules[\"type\"] == \"number\":\n",
    "        try:\n",
    "            if isinstance(field_value, str):\n",
    "                field_value = field_value.replace(\".\", \"\").replace(\",\", \".\")\n",
    "            if float(field_value) > rules[\"max\"]:\n",
    "                validation_result[\"is_valid\"] = False\n",
    "                validation_result[\"errors\"].append(f\"Value must be at most {rules['max']}\")\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    # Pattern validation\n",
    "    if \"pattern\" in rules:\n",
    "        import re\n",
    "        if not re.match(rules[\"pattern\"], str(field_value)):\n",
    "            validation_result[\"is_valid\"] = False\n",
    "            validation_result[\"errors\"].append(f\"Value does not match required pattern\")\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "def validate_extracted_fields(fields: Dict[str, Any], doc_config: DocumentTypeConfig) -> Dict[str, Any]:\n",
    "    \"\"\"Validate all extracted fields against their validation rules.\"\"\"\n",
    "    validation_results = {}\n",
    "    for field_name, field_data in fields.items():\n",
    "        if field_name in doc_config.validation_rules:\n",
    "            validation_results[field_name] = validate_field(field_data, doc_config.validation_rules[field_name])\n",
    "    return validation_results\n",
    "\n",
    "async def extract_fields_with_llm(\n",
    "    ocr_lines: List[Dict[str, Any]],\n",
    "    doc_config: DocumentTypeConfig,\n",
    "    original_ocr_lines: List[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract fields from OCR lines using LLM.\n",
    "    The LLM is only used to map OCR text to field names.\n",
    "    Original OCR data (value, confidence, bounding box, page) is preserved.\n",
    "    \n",
    "    Args:\n",
    "        ocr_lines: List of OCR lines with text and metadata\n",
    "        doc_config: Document type configuration\n",
    "        llm_client: LLM client for field extraction\n",
    "        original_ocr_lines: Optional list of original OCR lines for reference\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing extracted fields, missing fields, and validation results\n",
    "    \"\"\"\n",
    "    if not ocr_lines:\n",
    "        return {\n",
    "            \"extracted_fields\": {},\n",
    "            \"missing_fields\": list(doc_config.expected_fields),\n",
    "            \"validation_results\": {}\n",
    "        }\n",
    "        \n",
    "    # Step 1: Let LLM map OCR text to field names\n",
    "    prompt = create_extraction_prompt(ocr_lines, doc_config)\n",
    "    response = await llm_client.generate(prompt)\n",
    "    \n",
    "    try:\n",
    "        llm_result = extract_json_from_response(response)\n",
    "    except ValueError as e:\n",
    "        raise\n",
    "        \n",
    "    # Step 2: Process extracted fields\n",
    "    extracted_fields = {}\n",
    "    for field_name, field_data in llm_result.get(\"extracted_fields\", {}).items():\n",
    "        # Ensure field data is a dictionary\n",
    "        if not isinstance(field_data, dict):\n",
    "            field_data = {\"value\": field_data}\n",
    "            \n",
    "        # Ensure required keys exist\n",
    "        if \"value\" not in field_data:\n",
    "            field_data[\"value\"] = None\n",
    "            \n",
    "        # Step 3: Find matching normalized label-value pair\n",
    "        if field_data[\"value\"] is not None:\n",
    "            value_str = str(field_data[\"value\"]).lower()\n",
    "            \n",
    "            # Get all possible labels for this field\n",
    "            # Use the DB key and its human description as candidate labels\n",
    "            df_field_names = []\n",
    "            try:\n",
    "                field_desc = doc_config.field_descriptions.get(field_name, \"\")\n",
    "            except AttributeError:\n",
    "                # If doc_config is a dict\n",
    "                field_desc = (doc_config.get(\"field_descriptions\", {}) or {}).get(field_name, \"\")\n",
    "            df_field_names = [field_name.lower()]\n",
    "            if field_desc:\n",
    "                df_field_names.append(str(field_desc).lower())\n",
    "            \n",
    "            # First try to find a matching label-value pair\n",
    "            matching_pair = None\n",
    "            for line in ocr_lines:\n",
    "                if line[\"type\"] == \"label_value\":\n",
    "                    line_label = line[\"label\"].lower()\n",
    "                    line_value = line[\"value\"].lower()\n",
    "                    \n",
    "                    # Match if either the label or value matches\n",
    "                    if (any(label in line_label for label in df_field_names) or \n",
    "                        value_str in line_value):\n",
    "                        matching_pair = line\n",
    "                        break\n",
    "            \n",
    "            if matching_pair:\n",
    "                # Use the label-value pair's data directly\n",
    "                extracted_fields[field_name] = {\n",
    "                    \"value\": matching_pair[\"value\"],\n",
    "                    \"confidence\": matching_pair.get(\"confidence\", 0.5),\n",
    "                    \"bounding_box\": matching_pair.get(\"bounding_box\"),\n",
    "                    \"page\": matching_pair.get(\"page\")\n",
    "                }\n",
    "            else:\n",
    "                # If no matching pair found, try to find matching OCR line\n",
    "                matching_line = None\n",
    "                if original_ocr_lines:\n",
    "                    for line in original_ocr_lines:\n",
    "                        line_text = line[\"text\"].lower()\n",
    "                        \n",
    "                        # Match if line contains either the value or any of the field's labels\n",
    "                        if value_str in line_text or any(label in line_text for label in df_field_names):\n",
    "                            matching_line = line\n",
    "                            break\n",
    "                \n",
    "                if matching_line:\n",
    "                    # Use the OCR line's data directly\n",
    "                    extracted_fields[field_name] = {\n",
    "                        \"value\": matching_line[\"text\"],\n",
    "                        \"confidence\": matching_line.get(\"confidence\", 0.5),\n",
    "                        \"bounding_box\": matching_line.get(\"bounding_box\"),\n",
    "                        \"page\": matching_line.get(\"page\")\n",
    "                    }\n",
    "                else:\n",
    "                    # If no matching line found, use LLM output with default confidence\n",
    "                    extracted_fields[field_name] = {\n",
    "                        \"value\": field_data[\"value\"],\n",
    "                        \"confidence\": 0.5\n",
    "                    }\n",
    "        else:\n",
    "            # If no value provided, use LLM output with default confidence\n",
    "            extracted_fields[field_name] = {\n",
    "                \"value\": field_data[\"value\"],\n",
    "                \"confidence\": 0.5\n",
    "            }\n",
    "            \n",
    "    # Step 4: Apply field mappings\n",
    "    mapped_fields = extracted_fields\n",
    "            \n",
    "    # Step 5: Validate fields\n",
    "    validation_results = validate_extracted_fields(mapped_fields, doc_config)\n",
    "    \n",
    "    # Prepare final result\n",
    "    result = {\n",
    "        \"extracted_fields\": mapped_fields,\n",
    "        \"missing_fields\": llm_result.get(\"missing_fields\", []),\n",
    "        \"validation_results\": validation_results\n",
    "    }\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "storage-section",
   "metadata": {},
   "source": [
    "## 3. Load OCR Data from Blob Storage\n",
    "\n",
    "Now let's load the OCR output that was stored by notebook 02 and apply our LLM functions to extract fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282d773",
   "metadata": {},
   "source": [
    "**Document Processing State Machine**\n",
    "\n",
    "We use a state machine to track documents through the processing pipeline:\n",
    "\n",
    "**States:**\n",
    "- **`RAW`**: Raw document input (PDFs, images)\n",
    "- **`OCR`**: Documents processed with OCR, ready for LLM analysis\n",
    "- **`LLM`**: Documents processed by LLM, fields extracted and validated\n",
    "\n",
    "**Why Use This Pattern:**\n",
    "- **Traceability**: Track each document's processing status\n",
    "- **Storage Organization**: Separate containers for each stage\n",
    "- **Error Recovery**: Can restart from any stage if processing fails\n",
    "- **Scalability**: Easy to add new processing stages\n",
    "\n",
    "**Implementation:**\n",
    "- Each stage has its own blob storage container\n",
    "- Documents move through stages as they're processed\n",
    "- Thread-safe singleton ensures consistent state management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88a568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local connection string to Azurite\n",
    "connection_string = \"DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "storage-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from enum import Enum\n",
    "from pathlib import PurePosixPath\n",
    "from typing import Optional\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "\n",
    "\n",
    "class Stage(Enum):\n",
    "    \"\"\"Processing stages for credit documents.\"\"\"\n",
    "    RAW = \"raw\"\n",
    "    OCR = \"ocr\"\n",
    "    LLM = \"llm\"\n",
    "\n",
    "\n",
    "class BlobStorage:\n",
    "    \"\"\"Thread-safe singleton for blob storage operations.\"\"\"\n",
    "    \n",
    "    _instance: Optional['BlobStorage'] = None\n",
    "    _lock = threading.Lock()\n",
    "    \n",
    "    def __new__(cls):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        if hasattr(self, '_initialized'):\n",
    "            return\n",
    "        \n",
    "        self._connection_string = connection_string\n",
    "        self._blob_service_client = None\n",
    "        self._initialized_containers = set()\n",
    "        self._container_lock = threading.Lock()\n",
    "        self._initialized = True\n",
    "    \n",
    "    @property\n",
    "    def blob_service_client(self) -> BlobServiceClient:\n",
    "        \"\"\"Get blob service client, initializing if needed.\"\"\"\n",
    "        if self._blob_service_client is None:\n",
    "            self._blob_service_client = BlobServiceClient.from_connection_string(self._connection_string)\n",
    "        return self._blob_service_client\n",
    "    \n",
    "    def _ensure_container_exists(self, container_name: str) -> None:\n",
    "        \"\"\"Ensure a specific container exists.\"\"\"\n",
    "        if container_name in self._initialized_containers:\n",
    "            return\n",
    "            \n",
    "        with self._container_lock:\n",
    "            if container_name in self._initialized_containers:\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                container_client = self.blob_service_client.get_container_client(container_name)\n",
    "                container_client.create_container()\n",
    "            except ResourceExistsError:\n",
    "                pass\n",
    "            \n",
    "            self._initialized_containers.add(container_name)\n",
    "    \n",
    "    def blob_exists(self, uuid: str, stage: Stage, ext: str) -> bool:\n",
    "        \"\"\"Check if a blob exists at a specific stage.\"\"\"\n",
    "        try:\n",
    "            container_name = stage.value\n",
    "            self._ensure_container_exists(container_name)\n",
    "            container_client = self.blob_service_client.get_container_client(container_name)\n",
    "            blob_client = container_client.get_blob_client(f\"{uuid}{ext}\")\n",
    "            blob_client.get_blob_properties()\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def download_blob(self, uuid: str, stage: Stage, ext: str) -> Optional[bytes]:\n",
    "        \"\"\"Download data from a blob at a specific stage.\"\"\"\n",
    "        try:\n",
    "            container_name = stage.value\n",
    "            self._ensure_container_exists(container_name)\n",
    "            container_client = self.blob_service_client.get_container_client(container_name)\n",
    "            blob_client = container_client.get_blob_client(f\"{uuid}{ext}\")\n",
    "            blob_data = blob_client.download_blob()\n",
    "            data = blob_data.readall()\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download blob: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def list_blobs_in_stage(self, stage: Stage) -> list[str]:\n",
    "        \"\"\"List all blobs in a specific stage container.\"\"\"\n",
    "        container_name = stage.value\n",
    "        self._ensure_container_exists(container_name)\n",
    "        container_client = self.blob_service_client.get_container_client(container_name)\n",
    "        \n",
    "        blob_names = []\n",
    "        try:\n",
    "            blob_list = container_client.list_blobs()\n",
    "            for blob in blob_list:\n",
    "                blob_names.append(blob.name)\n",
    "            return blob_names\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to list blobs: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "def get_storage() -> BlobStorage:\n",
    "    \"\"\"Get the singleton BlobStorage instance.\"\"\"\n",
    "    return BlobStorage()\n",
    "\n",
    "\n",
    "def read_ocr_results_from_bucket(document_uuid: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Read OCR results from blob storage bucket.\"\"\"\n",
    "    storage_client = get_storage()\n",
    "    \n",
    "    if not storage_client.blob_exists(document_uuid, Stage.OCR, \".json\"):\n",
    "        print(f\"OCR results not found for document: {document_uuid}\")\n",
    "        return None\n",
    "    \n",
    "    blob_data = storage_client.download_blob(document_uuid, Stage.OCR, \".json\")\n",
    "    \n",
    "    if blob_data is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        json_string = blob_data.decode('utf-8')\n",
    "        ocr_data = json.loads(json_string)\n",
    "        return ocr_data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse OCR results: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def list_ocr_results_in_bucket() -> list[str]:\n",
    "    \"\"\"List all OCR result files in the bucket.\"\"\"\n",
    "    storage_client = get_storage()\n",
    "    \n",
    "    try:\n",
    "        blob_names = storage_client.list_blobs_in_stage(Stage.OCR)\n",
    "        \n",
    "        document_uuids = []\n",
    "        for blob_name in blob_names:\n",
    "            if blob_name.endswith('.json'):\n",
    "                uuid_part = blob_name.replace('.json', '')\n",
    "                document_uuids.append(uuid_part)\n",
    "        \n",
    "        return document_uuids\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to list OCR results: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f45909",
   "metadata": {},
   "source": [
    "**Practical implementation of the storage functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "load-ocr-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for OCR results in blob storage...\n",
      "Found 2 stored documents:\n",
      "  1. 945a6da1-a563-434e-ac6a-b992166b17fe\n",
      "  2. de196c17-0f61-4a03-96b4-b17b0fd4102c\n",
      "\n",
      "Loading OCR results for document: 945a6da1-a563-434e-ac6a-b992166b17fe\n",
      "Successfully loaded OCR data from blob storage\n",
      "Document UUID: 945a6da1-a563-434e-ac6a-b992166b17fe\n",
      "Timestamp: 2025-09-04T22:15:14.746600\n",
      "\n",
      "OCR Data Structure:\n",
      "  - Normalized lines: 43\n",
      "  - Label-value pairs: 26\n",
      "  - Text lines: 17\n",
      "\n",
      "Sample label-value pairs:\n",
      "  1. 1 → Applicant\n",
      "  2. Company Name → DemoTech Solutions GmbH\n",
      "  3. Legal Form → Limited Liability Company (Gmb...\n",
      "  4. Date of Incorporation → 12/05/2018\n",
      "  5. Business Address → Main Street 123, 70173 Stuttga...\n"
     ]
    }
   ],
   "source": [
    "# Find the OCR file with stage \"ocr\" metadata\n",
    "print(\"Looking for OCR results in blob storage...\")\n",
    "stored_documents = list_ocr_results_in_bucket()\n",
    "\n",
    "if not stored_documents:\n",
    "    print(\"No OCR results found in storage. Please run notebook 02 first to generate OCR data.\")\n",
    "else:\n",
    "    print(f\"Found {len(stored_documents)} stored documents:\")\n",
    "    for i, doc_uuid in enumerate(stored_documents):\n",
    "        print(f\"  {i+1}. {doc_uuid}\")\n",
    "    \n",
    "    # Load the first (and only) document\n",
    "    document_uuid = stored_documents[0]\n",
    "    print(f\"\\nLoading OCR results for document: {document_uuid}\")\n",
    "    \n",
    "    ocr_data = read_ocr_results_from_bucket(document_uuid)\n",
    "    \n",
    "    if ocr_data:\n",
    "        print(\"Successfully loaded OCR data from blob storage\")\n",
    "        print(f\"Document UUID: {ocr_data['document_uuid']}\")\n",
    "        print(f\"Timestamp: {ocr_data['timestamp']}\")\n",
    "        \n",
    "        # Extract the OCR results\n",
    "        stored_ocr_results = ocr_data['ocr_results']\n",
    "        normalized_lines = stored_ocr_results['normalized_lines']\n",
    "        \n",
    "        print(f\"\\nOCR Data Structure:\")\n",
    "        print(f\"  - Normalized lines: {len(normalized_lines)}\")\n",
    "        \n",
    "        # Count different types\n",
    "        label_value_count = sum(1 for item in normalized_lines if item['type'] == 'label_value')\n",
    "        text_line_count = sum(1 for item in normalized_lines if item['type'] == 'text_line')\n",
    "        \n",
    "        print(f\"  - Label-value pairs: {label_value_count}\")\n",
    "        print(f\"  - Text lines: {text_line_count}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nSample label-value pairs:\")\n",
    "        label_value_pairs = [item for item in normalized_lines if item['type'] == 'label_value']\n",
    "        for i, pair in enumerate(label_value_pairs[:5]):\n",
    "            print(f\"  {i+1}. {pair['label']} → {pair['value'][:30]}{'...' if len(pair['value']) > 30 else ''}\")\n",
    "    else:\n",
    "        print(\"Failed to load OCR data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-application",
   "metadata": {},
   "source": [
    "## 4. Apply LLM Functions to Extract Fields\n",
    "\n",
    "Now let's apply our LLM functions to the loaded OCR data to extract structured fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "apply-llm-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LLM field extraction to OCR data...\n",
      "LLM field extraction completed successfully!\n",
      "\n",
      "Extraction Results:\n",
      "  - Extracted fields: 21\n",
      "  - Missing fields: 0\n",
      "  - Validation results: 21\n",
      "\n",
      "Extracted Fields:\n",
      "  • company_name: DemoTech Solutions GmbH (confidence: 0.994)\n",
      "  • legal_form: Limited Liability Company (GmbH) (confidence: 0.803)\n",
      "  • founding_date: 12.05.2018 (confidence: 0.500)\n",
      "  • business_address: Main Street 123, 70173 Stuttgart, Germany (confidence: 0.721)\n",
      "  • commercial_register: HRB 123456 / Stuttgart Local Court (confidence: 0.978)\n",
      "  • vat_id: DE123456789 (confidence: 0.919)\n",
      "  • property_type: Office and Commercial Building (confidence: 0.995)\n",
      "  • property_name: Innovation Center Stuttgart (confidence: 0.595)\n",
      "  • property_address: Tech Park 45, 70191 Stuttgart, Germany (confidence: 0.683)\n",
      "  • purchase_price: €2,500,000 (confidence: 0.500)\n",
      "  • requested_amount: €2,000,000 (confidence: 0.715)\n",
      "  • purpose: Purchase and Renovation (confidence: 0.827)\n",
      "  • equity_share: €500,000 (confidence: 0.500)\n",
      "  • construction_year: 2015 (confidence: 0.991)\n",
      "  • total_area: 2,800 m² (confidence: 0.500)\n",
      "  • loan_amount: €2,000,000 (confidence: 0.715)\n",
      "  • term: 15 years (confidence: 1.000)\n",
      "  • monthly_payment: Applicant (confidence: 1.000)\n",
      "  • interest_rate: Applicant (confidence: 1.000)\n",
      "  • early_repayment: False (confidence: 0.500)\n",
      "  • public_funding: True (confidence: 0.500)\n",
      "\n",
      "Validation Results:\n",
      "  • company_name: Valid\n",
      "  • legal_form: Valid\n",
      "  • founding_date: Valid\n",
      "  • business_address: Valid\n",
      "  • commercial_register: Valid\n",
      "  • vat_id: Valid\n",
      "  • property_type: Valid\n",
      "  • property_name: Valid\n",
      "  • property_address: Valid\n",
      "  • purchase_price: Valid\n",
      "  • requested_amount: Valid\n",
      "  • purpose: Valid\n",
      "  • equity_share: Valid\n",
      "  • construction_year: Valid\n",
      "  • total_area: Valid\n",
      "  • loan_amount: Valid\n",
      "  • term: Valid\n",
      "  • monthly_payment: Invalid\n",
      "    - Value does not match required pattern\n",
      "  • interest_rate: Invalid\n",
      "    - Value does not match required pattern\n",
      "  • early_repayment: Valid\n",
      "  • public_funding: Valid\n"
     ]
    }
   ],
   "source": [
    "# Apply LLM extraction to the loaded OCR data\n",
    "if 'normalized_lines' in locals():\n",
    "    print(\"Applying LLM field extraction to OCR data...\")\n",
    "    \n",
    "    # Use the existing LLM client and document config\n",
    "    # try:\n",
    "        # Extract fields using LLM\n",
    "    extraction_result = await extract_fields_with_llm(\n",
    "        ocr_lines=normalized_lines,\n",
    "        doc_config=doc_config[\"credit_request\"],\n",
    "        original_ocr_lines=stored_ocr_results.get('original_lines', [])\n",
    "    )\n",
    "    \n",
    "    print(\"LLM field extraction completed successfully!\")\n",
    "    \n",
    "    # Display results\n",
    "    extracted_fields = extraction_result['extracted_fields']\n",
    "    missing_fields = extraction_result['missing_fields']\n",
    "    validation_results = extraction_result['validation_results']\n",
    "    \n",
    "    print(f\"\\nExtraction Results:\")\n",
    "    print(f\"  - Extracted fields: {len(extracted_fields)}\")\n",
    "    print(f\"  - Missing fields: {len(missing_fields)}\")\n",
    "    print(f\"  - Validation results: {len(validation_results)}\")\n",
    "    \n",
    "    print(f\"\\nExtracted Fields:\")\n",
    "    for field_name, field_data in extracted_fields.items():\n",
    "        value = field_data.get('value', 'N/A')\n",
    "        confidence = field_data.get('confidence', 0.0)\n",
    "        print(f\"  • {field_name}: {value} (confidence: {confidence:.3f})\")\n",
    "    \n",
    "    if missing_fields:\n",
    "        print(f\"\\nMissing Fields:\")\n",
    "        for field in missing_fields:\n",
    "            print(f\"  • {field}\")\n",
    "    \n",
    "    if validation_results:\n",
    "        print(f\"\\nValidation Results:\")\n",
    "        for field_name, validation in validation_results.items():\n",
    "            is_valid = validation.get('is_valid', False)\n",
    "            errors = validation.get('errors', [])\n",
    "            status = \"Valid\" if is_valid else \"Invalid\"\n",
    "            print(f\"  • {field_name}: {status}\")\n",
    "            if errors:\n",
    "                for error in errors:\n",
    "                    print(f\"    - {error}\")\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error during LLM extraction: {e}\")\n",
    "else:\n",
    "    print(\"No OCR data loaded. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-llm-results",
   "metadata": {},
   "source": [
    "## 5. Save LLM Results to Blob Storage\n",
    "\n",
    "Let's save the LLM extraction results back to blob storage for the next processing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "save-llm-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LLM extraction results to blob storage...\n",
      "LLM results saved to blob storage: llm/945a6da1-a563-434e-ac6a-b992166b17fe.json\n",
      "Verification: LLM results confirmed in storage\n"
     ]
    }
   ],
   "source": [
    "# Save LLM results to blob storage\n",
    "if 'extraction_result' in locals():\n",
    "    print(\"Saving LLM extraction results to blob storage...\")\n",
    "    \n",
    "    # Prepare LLM results data\n",
    "    llm_results = {\n",
    "        \"document_uuid\": document_uuid,\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"llm_results\": extraction_result,\n",
    "        \"metadata\": {\n",
    "            \"source_ocr_uuid\": document_uuid,\n",
    "            \"stage\": \"llm\",\n",
    "            \"model_name\": MODEL_NAME,\n",
    "            \"extraction_method\": \"llm_field_extraction\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Convert to JSON and save\n",
    "    try:\n",
    "        llm_data_bytes = json.dumps(llm_results, indent=2, ensure_ascii=False).encode('utf-8')\n",
    "        \n",
    "        storage_client = get_storage()\n",
    "        container_name = Stage.LLM.value\n",
    "        storage_client._ensure_container_exists(container_name)\n",
    "        container_client = storage_client.blob_service_client.get_container_client(container_name)\n",
    "        blob_client = container_client.get_blob_client(f\"{document_uuid}.json\")\n",
    "        blob_client.upload_blob(llm_data_bytes, overwrite=True)\n",
    "        \n",
    "        print(f\"LLM results saved to blob storage: {container_name}/{document_uuid}.json\")\n",
    "        \n",
    "        # Verify the save\n",
    "        if storage_client.blob_exists(document_uuid, Stage.LLM, \".json\"):\n",
    "            print(\"Verification: LLM results confirmed in storage\")\n",
    "        else:\n",
    "            print(\"Verification failed: LLM results not found in storage\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving LLM results: {e}\")\n",
    "else:\n",
    "    print(\"No LLM extraction results to save. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "**Loaded OCR Data**: Retrieved structured OCR results from blob storage\n",
    "**LLM Field Extraction**: Applied LLM functions to extract structured fields\n",
    "**Field Validation**: Validated extracted fields against business rules\n",
    "**Results Storage**: Saved LLM results back to blob storage\n",
    "**Complete Pipeline**: Connected OCR processing with LLM analysis\n",
    "\n",
    "### Processing Pipeline\n",
    "\n",
    "1. **OCR Processing** (Notebook 02) → Extract text with spatial analysis\n",
    "2. **Storage** (Notebook 02) → Save OCR results to blob storage\n",
    "3. **Data Loading** (This notebook) → Retrieve OCR data from storage\n",
    "4. **LLM Analysis** (This notebook) → Extract structured fields using LLM\n",
    "5. **Results Storage** (This notebook) → Save LLM results for next stage\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Database Integration**: Store extracted fields in PostgreSQL\n",
    "- **API Development**: Create endpoints to serve processed data\n",
    "- **Frontend Integration**: Display results with confidence scores\n",
    "- **Validation Workflows**: Implement manual review processes\n",
    "\n",
    "The document processing pipeline is now complete from OCR extraction through LLM analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
