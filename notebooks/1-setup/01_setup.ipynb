{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# 1. Setup Guide\n",
    "\n",
    "This notebook contains the executable setup steps for the Credit OCR System infrastructure.\n",
    "\n",
    "> **ðŸ“– For detailed explanations** of the system architecture, technology choices, and learning resources, see the [README.md](./README.md) in this folder.\n",
    "\n",
    "## Setup Overview\n",
    "\n",
    "We'll deploy and configure four services:\n",
    "- **PostgreSQL** (database) â†’ **Redis** (message broker) â†’ **Ollama** (AI models) â†’ **Azurite** (file storage)\n",
    "\n",
    "**Estimated time:** 20-30 minutes first run, 2-3 minutes subsequent runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jbe3c0gz31c",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "**Required:** Python 3.10+, Docker Desktop, UV package manager  \n",
    "**Optional, but recommended:** Git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisites",
   "metadata": {},
   "source": [
    "### 1.1 Quick Setup Check\n",
    "\n",
    "**Minimum:** 8GB RAM, 15GB disk space  \n",
    "**Recommended:** 16GB RAM, 25GB disk space\n",
    "\n",
    "> **âš ï¸ Start Docker Desktop** before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-intro",
   "metadata": {},
   "source": [
    "### 1.2 Initial Project Setup\n",
    "\n",
    "### Step-by-Step Project Setup\n",
    "\n",
    "Follow these steps to prepare your development environment:\n",
    "\n",
    "**1. Project Preparation** (2 minutes)\n",
    "```bash\n",
    "# Navigate to your desired directory\n",
    "cd /path/to/your/projects\n",
    "\n",
    "# Download/clone the project (if not done already)\n",
    "# Ensure the project folder contains compose.yml\n",
    "```\n",
    "\n",
    "**2. Python Environment Setup** (3-5 minutes)\n",
    "```bash\n",
    "# Navigate to project root (should contain compose.yml)\n",
    "cd credit-ocr-system\n",
    "\n",
    "# Create isolated Python environment\n",
    "uv venv\n",
    "\n",
    "# Install all project dependencies\n",
    "uv sync\n",
    "```\n",
    "\n",
    "**3. Jupyter Notebook Launch** (1 minute)\n",
    "```bash\n",
    "# Start Jupyter with project environment\n",
    "uv run jupyter notebook\n",
    "\n",
    "# Open this notebook (01_setup.ipynb)\n",
    "# Verify kernel shows \".venv\" or project name\n",
    "```\n",
    "\n",
    "### Understanding Virtual Environments\n",
    "\n",
    "**What is a virtual environment?**\n",
    "A virtual environment creates an isolated Python installation for this project. This means:\n",
    "- âœ… **No conflicts**: Project dependencies won't interfere with your system Python\n",
    "- âœ… **Reproducible**: Same environment on every machine\n",
    "- âœ… **Clean**: Easy to reset or remove if needed\n",
    "\n",
    "**How to verify it's working:**\n",
    "- Jupyter kernel should show `.venv` or your project name\n",
    "- Running `which python` in terminal should show the `.venv` path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48baed70",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration\n",
    "\n",
    "### 2.1 Import Required Libraries\n",
    "\n",
    "Before we begin the technical setup, let's import all the Python libraries we'll need. This consolidates our dependencies and makes it clear what tools we're using.\n",
    "\n",
    "### Why We Need These Libraries\n",
    "- **Standard Library**: Built-in Python modules for system operations, file handling, and process management\n",
    "- **Third-party Libraries**: External packages for HTTP requests and database/cache connections\n",
    "- **Optional Imports**: Libraries that enhance functionality but aren't strictly required for basic operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "980df090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "   Redis library available: True\n",
      "   PostgreSQL library available: True\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party Library Imports\n",
    "import requests\n",
    "\n",
    "# Optional imports (will check availability later)\n",
    "try:\n",
    "    import redis\n",
    "    REDIS_AVAILABLE: bool = True\n",
    "except ImportError:\n",
    "    REDIS_AVAILABLE: bool = False\n",
    "\n",
    "try:\n",
    "    import psycopg2\n",
    "    PSYCOPG2_AVAILABLE: bool = True\n",
    "except ImportError:\n",
    "    PSYCOPG2_AVAILABLE: bool = False\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"   Redis library available: {REDIS_AVAILABLE}\")\n",
    "print(f\"   PostgreSQL library available: {PSYCOPG2_AVAILABLE}\")\n",
    "\n",
    "if not REDIS_AVAILABLE or not PSYCOPG2_AVAILABLE:\n",
    "    print(\"\\nSome libraries missing. Run 'uv sync' to install them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f59b7",
   "metadata": {},
   "source": [
    "### 2.2 System Configuration & Environment Validation\n",
    "\n",
    "Before we begin the setup, let's define the configuration settings that will be used throughout this notebook. Understanding these settings helps you see how our microservices will communicate.\n",
    "\n",
    "### Configuration Strategy\n",
    "- **Development Mode**: We're using simple constants for easy understanding\n",
    "- **Production Ready**: These values align with our `compose.yml` and will later move to a dedicated config file\n",
    "- **Port Mapping**: Each service uses a specific port to avoid conflicts\n",
    "- **Local Focus**: All services run on localhost for secure, local development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b0b29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "   Generative Model: llama3.1:8b\n",
      "   Generative Model URL: http://127.0.0.1:11435\n",
      "   Database: dms_meta on localhost:5432\n",
      "   Message Broker (Redis): localhost:6379\n",
      "   File Storage: localhost:10000\n",
      "\n",
      "All services configured for local development\n",
      "All data stays on your machine - no external dependencies\n"
     ]
    }
   ],
   "source": [
    "# Simple Configuration Settings\n",
    "# These are the basic settings we'll use throughout this setup\n",
    "\n",
    "# AI Model Configuration (will be moved to config file later)\n",
    "GENERATIVE_MODEL_URL = \"http://127.0.0.1:11435\"\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "\n",
    "# Database Settings (from compose.yml)\n",
    "DATABASE_HOST = \"localhost\"\n",
    "DATABASE_PORT = 5432\n",
    "DATABASE_NAME = \"dms_meta\"\n",
    "DATABASE_USER = \"dms\"\n",
    "DATABASE_PASSWORD = \"dms\"\n",
    "\n",
    "# Other Service Settings\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "AZURITE_HOST = \"localhost\" \n",
    "AZURITE_PORT = 10000\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"   Generative Model: {MODEL_NAME}\")\n",
    "print(f\"   Generative Model URL: {GENERATIVE_MODEL_URL}\")\n",
    "print(f\"   Database: {DATABASE_NAME} on {DATABASE_HOST}:{DATABASE_PORT}\")\n",
    "print(f\"   Message Broker (Redis): {REDIS_HOST}:{REDIS_PORT}\")\n",
    "print(f\"   File Storage: {AZURITE_HOST}:{AZURITE_PORT}\")\n",
    "print()\n",
    "print(\"All services configured for local development\")\n",
    "print(\"All data stays on your machine - no external dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74qckgs5icl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.10.16\n",
      "Python Path: /Users/markuskuehnle/Documents/projects/credit-ocr-system/.venv/bin/python\n",
      "Python version is compatible\n"
     ]
    }
   ],
   "source": [
    "# Check Python Environment\n",
    "python_version = sys.version_info\n",
    "print(f\"Python Version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "print(f\"Python Path: {sys.executable}\")\n",
    "\n",
    "if python_version >= (3, 10):\n",
    "    print(\"Python version is compatible\")\n",
    "else:\n",
    "    print(\"Error: Python 3.10 or higher is required\")\n",
    "    print(\"Please install a newer Python version and try again\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12izzsax7tmq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found project root: /Users/markuskuehnle/Documents/projects/credit-ocr-system\n",
      "Found compose.yml file\n"
     ]
    }
   ],
   "source": [
    "# Find Project Root Directory\n",
    "current_directory = Path.cwd()\n",
    "\n",
    "# Check if we are in the notebook subdirectory\n",
    "if current_directory.name == \"1-setup\" and current_directory.parent.name == \"notebooks\":\n",
    "    project_root_directory = current_directory.parent.parent\n",
    "# Check if we are already in the project root\n",
    "elif (current_directory / \"compose.yml\").exists():\n",
    "    project_root_directory = current_directory\n",
    "else:\n",
    "    project_root_directory = None\n",
    "\n",
    "if project_root_directory and (project_root_directory / \"compose.yml\").exists():\n",
    "    print(f\"Found project root: {project_root_directory}\")\n",
    "    print(\"Found compose.yml file\")\n",
    "else:\n",
    "    print(\"Error: Cannot find compose.yml file\")\n",
    "    print(\"Make sure you are running this notebook from the correct directory\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "step2-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker is installed: Docker version 28.0.1, build 068a01e\n"
     ]
    }
   ],
   "source": [
    "# Check Docker Installation\n",
    "try:\n",
    "    docker_version_result = subprocess.run([\"docker\", \"--version\"], capture_output=True, text=True, timeout=5)\n",
    "    if docker_version_result.returncode == 0:\n",
    "        version_output = docker_version_result.stdout.strip()\n",
    "        print(f\"Docker is installed: {version_output}\")\n",
    "    else:\n",
    "        print(\"Docker is installed but not working properly\")\n",
    "        print(\"Try restarting Docker Desktop\")\n",
    "        exit()\n",
    "except FileNotFoundError:\n",
    "    print(\"Docker is not installed\")\n",
    "    print(\"Please install Docker Desktop from https://docs.docker.com/get-docker/\")\n",
    "    exit()\n",
    "except Exception as error:\n",
    "    print(f\"âœ— Error checking Docker: {error}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cue2b9j33ho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker Compose is available: v2.33.1-desktop.1\n"
     ]
    }
   ],
   "source": [
    "# Check Docker Compose\n",
    "try:\n",
    "    compose_version_result = subprocess.run([\"docker\", \"compose\", \"version\"], capture_output=True, text=True, timeout=5)\n",
    "    if compose_version_result.returncode == 0:\n",
    "        compose_version = compose_version_result.stdout.split()[3]\n",
    "        print(f\"Docker Compose is available: {compose_version}\")\n",
    "    else:\n",
    "        print(\"Docker Compose is not working properly\")\n",
    "        print(\"Make sure Docker Desktop is running\")\n",
    "        exit()\n",
    "except FileNotFoundError:\n",
    "    print(\"Docker Compose command not found\")\n",
    "    print(\"Docker Compose should come with Docker Desktop\")\n",
    "    exit()\n",
    "except Exception as error:\n",
    "    print(f\"âœ— Error checking Docker Compose: {error}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "k6oz19mcke8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UV package manager is installed: uv 0.7.19 (38ee6ec80 2025-07-02)\n",
      "\n",
      "All required software is ready!\n",
      "\n",
      "Next step: Start the services with Docker Compose\n"
     ]
    }
   ],
   "source": [
    "# Check UV Package Manager\n",
    "try:\n",
    "    uv_version_result = subprocess.run([\"uv\", \"--version\"], capture_output=True, text=True, timeout=5)\n",
    "    if uv_version_result.returncode == 0:\n",
    "        uv_version = uv_version_result.stdout.strip()\n",
    "        print(f\"UV package manager is installed: {uv_version}\")\n",
    "        print(\"\\nAll required software is ready!\")\n",
    "        print(\"\\nNext step: Start the services with Docker Compose\")\n",
    "    else:\n",
    "        print(\"UV package manager is not working properly\")\n",
    "        exit()\n",
    "except FileNotFoundError:\n",
    "    print(\"UV package manager is not installed\")\n",
    "    print(\"Please install UV from https://docs.astral.sh/uv/getting-started/installation/\")\n",
    "    exit()\n",
    "except Exception as error:\n",
    "    print(f\"Error checking UV: {error}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bk2oh59la",
   "metadata": {},
   "source": [
    "## 3. Service Deployment & Health Monitoring\n",
    "\n",
    "### 3.1 Service Orchestration with Docker Compose\n",
    "\n",
    "Now we'll start all the services that make up our Credit OCR system. Docker Compose will coordinate four different services to work together seamlessly.\n",
    "\n",
    "### Understanding Docker Compose\n",
    "Docker Compose reads our `compose.yml` file and:\n",
    "- **Downloads images** (only on first run - may take 5-10 minutes)\n",
    "- **Creates networks** for services to communicate securely\n",
    "- **Starts containers** in the correct order with health checks\n",
    "- **Manages dependencies** ensuring services start when their requirements are met\n",
    "\n",
    "### The Four Services We're Starting\n",
    "| Service | Purpose | First-time Download | Startup Time |\n",
    "|---------|---------|-------------------|--------------|\n",
    "| **PostgreSQL** | Document metadata storage | ~200MB | ~15 seconds |\n",
    "| **Redis** | Message broker for background jobs | ~50MB | ~5 seconds |\n",
    "| **Ollama** | Local AI model server | ~500MB | ~30 seconds |\n",
    "| **Azurite** | Local file storage emulator | ~100MB | ~10 seconds |\n",
    "\n",
    "### Alternative Manual Method\n",
    "If you prefer terminal commands, you can start services manually:\n",
    "\n",
    "```bash\n",
    "cd /path/to/your/project/root\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "**First-time setup note:** Downloads may take several minutes depending on internet speed. This is normal and only happens once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "l4vhkj6bl7h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker is running and accessible\n"
     ]
    }
   ],
   "source": [
    "# Check if Docker is Running\n",
    "try:\n",
    "    docker_info_result = subprocess.run([\"docker\", \"info\"], capture_output=True, timeout=5)\n",
    "    if docker_info_result.returncode == 0:\n",
    "        print(\"Docker is running and accessible\")\n",
    "    else:\n",
    "        print(\"Docker is not running\")\n",
    "        print(\"Please start Docker Desktop and try again\")\n",
    "        exit()\n",
    "except Exception as error:\n",
    "    print(\"Cannot access Docker\")\n",
    "    print(\"Make sure Docker Desktop is running\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab8928",
   "metadata": {},
   "source": [
    "We run `docker compose up -d` with the next cell to start services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45eb93d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All services started successfully\n"
     ]
    }
   ],
   "source": [
    "def start_docker_services() -> None:\n",
    "    \"\"\"Start all required docker services in detached mode\"\"\"\n",
    "    docker_up_result: subprocess.CompletedProcess = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"up\", \"-d\"],\n",
    "        cwd=str(project_root_directory),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if docker_up_result.returncode == 0:\n",
    "        print(\"All services started successfully\")\n",
    "    else:\n",
    "        print(\"Failed to start services\")\n",
    "        print(docker_up_result.stderr)\n",
    "\n",
    "\n",
    "start_docker_services()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1yuulcv2wqe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current service status:\n",
      "  âœ“ azurite: running\n",
      "  âœ“ ollama: running\n",
      "  âœ“ postgres: running\n",
      "  âœ“ redis: running\n"
     ]
    }
   ],
   "source": [
    "# Check Current Running Services\n",
    "try:\n",
    "    containers_result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"json\"],\n",
    "        cwd=str(project_root_directory),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if containers_result.returncode == 0 and containers_result.stdout.strip():\n",
    "        print(\"Current service status:\")\n",
    "        for line in containers_result.stdout.strip().split('\\n'):\n",
    "            if line.strip():\n",
    "                try:\n",
    "                    container_info = json.loads(line)\n",
    "                    service_name = container_info.get('Service', 'unknown')\n",
    "                    service_state = container_info.get('State', 'unknown')\n",
    "                    \n",
    "                    if service_state == 'running':\n",
    "                        print(f\"  âœ“ {service_name}: {service_state}\")\n",
    "                    else:\n",
    "                        print(f\"  âœ— {service_name}: {service_state}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    else:\n",
    "        print(\"No services are currently running\")\n",
    "        print(\"Run 'docker compose up -d' to start services\")\n",
    "        \n",
    "except Exception as error:\n",
    "    print(f\"Could not check service status: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ql8vfnm1iq",
   "metadata": {},
   "source": [
    "### 3.2 Service Health Monitoring\n",
    "\n",
    "Our Credit OCR system uses 4 services that work together. Let's check if they are all running properly:\n",
    "\n",
    "- **postgres**: Database for storing document metadata\n",
    "- **redis**: Cache for background job processing\n",
    "- **ollama**: AI model server for text analysis  \n",
    "- **azurite**: File storage for uploaded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77j1d8uyv9j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREDIT OCR SYSTEM - SERVICE STATUS\n",
      "======================================================================\n",
      "Service         State        Health       Description\n",
      "----------------------------------------------------------------------\n",
      "? azurite       running     starting    Azure Blob Storage emulator for file storage\n",
      "? ollama        running     starting    Local AI model server for text analysis\n",
      "? postgres      running     starting    PostgreSQL database for document storage\n",
      "? redis         running     starting    Redis cache for background job processing\n"
     ]
    }
   ],
   "source": [
    "# Check Each Service Health Status\n",
    "expected_services_for_credit_ocr = {\n",
    "    'postgres': 'PostgreSQL database for document storage',\n",
    "    'redis': 'Redis cache for background job processing',\n",
    "    'ollama': 'Local AI model server for text analysis',\n",
    "    'azurite': 'Azure Blob Storage emulator for file storage'\n",
    "}\n",
    "\n",
    "try:\n",
    "    service_status_result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"json\"],\n",
    "        cwd=str(project_root_directory),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=15\n",
    "    )\n",
    "    \n",
    "    if service_status_result.returncode == 0:\n",
    "        print(\"CREDIT OCR SYSTEM - SERVICE STATUS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"{'Service':<15} {'State':<12} {'Health':<12} {'Description'}\")\n",
    "        print(\"-\" * 70)\n",
    "    else:\n",
    "        print(\"Could not get service status\")\n",
    "        print(\"Make sure you ran 'docker compose up -d'\")\n",
    "        exit()\n",
    "        \n",
    "except Exception as error:\n",
    "    print(f\"Error checking services: {error}\")\n",
    "    exit()\n",
    "\n",
    "# Parse and display service status\n",
    "running_services = set()\n",
    "service_status_info = {}\n",
    "\n",
    "if service_status_result.stdout.strip():\n",
    "    for line in service_status_result.stdout.strip().split('\\n'):\n",
    "        if line.strip():\n",
    "            try:\n",
    "                container_data = json.loads(line)\n",
    "                service_name = container_data.get('Service', 'unknown')\n",
    "                service_state = container_data.get('State', 'unknown')\n",
    "                health_status = container_data.get('Health', 'no check')\n",
    "                \n",
    "                running_services.add(service_name)\n",
    "                service_status_info[service_name] = {'state': service_state, 'health': health_status}\n",
    "                \n",
    "                if service_state == 'running' and health_status in ['healthy', 'no check']:\n",
    "                    status_indicator = \"âœ“\"\n",
    "                    description = \"Ready for use\"\n",
    "                elif service_state == 'running' and health_status == 'unhealthy':\n",
    "                    status_indicator = \"âš \"\n",
    "                    description = \"Still starting up...\"\n",
    "                elif service_state == 'exited':\n",
    "                    status_indicator = \"âœ—\"\n",
    "                    description = \"Failed to start\"\n",
    "                else:\n",
    "                    status_indicator = \"?\"\n",
    "                    description = f\"Status: {service_state}\"\n",
    "                \n",
    "                service_description = expected_services_for_credit_ocr.get(service_name, \"Unknown service\")\n",
    "                print(f\"{status_indicator} {service_name:<13} {service_state:<11} {health_status:<11} {service_description}\")\n",
    "                \n",
    "            except json.JSONDecodeError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "393qfrwg7h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ All Credit OCR services are running successfully!\n",
      "Your system is ready for document processing.\n"
     ]
    }
   ],
   "source": [
    "# Check for Missing or Failed Services\n",
    "missing_services = set(expected_services_for_credit_ocr.keys()) - running_services\n",
    "\n",
    "if missing_services:\n",
    "    print(\"\\nMISSING SERVICES:\")\n",
    "    print(\"-\" * 50)\n",
    "    for missing_service in missing_services:\n",
    "        service_description = expected_services_for_credit_ocr[missing_service]\n",
    "        print(f\"âœ— {missing_service:<15} Not running - {service_description}\")\n",
    "\n",
    "failed_services = [service for service, info in service_status_info.items() \n",
    "                  if info['state'] in ['exited', 'restarting'] or info['health'] == 'unhealthy']\n",
    "\n",
    "if failed_services:\n",
    "    print(f\"\\nTROUBLESHOOTING FAILED SERVICES:\")\n",
    "    print(\"-\" * 50)\n",
    "    for failed_service in failed_services:\n",
    "        print(f\"Check logs: docker compose logs {failed_service}\")\n",
    "\n",
    "if missing_services or failed_services:\n",
    "    print(f\"\\nACTION NEEDED:\")\n",
    "    print(\"1. Start/restart services: docker compose up -d\")\n",
    "    print(\"2. Wait 1-2 minutes for services to start up\")\n",
    "    print(\"3. Re-run this cell to check status again\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ All Credit OCR services are running successfully!\")\n",
    "    print(\"Your system is ready for document processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4d380",
   "metadata": {},
   "source": [
    "## 4. AI Model Installation & Configuration\n",
    "\n",
    "Now that Ollama is running, let's install the AI model we'll use for credit document analysis. We'll use **Llama3.1:8b**, a powerful model that's excellent for financial document processing while still being efficient enough to run locally.\n",
    "\n",
    "**About Llama3.1:8b:**\n",
    "- Size: ~4.7GB download\n",
    "- Memory usage: ~8GB RAM when loaded\n",
    "- Performance: Excellent for text analysis and information extraction\n",
    "- Speed: Fast enough for real-time document processing\n",
    "\n",
    "> The next cell will download and install the model if needed. This process may take several minutes, especially on a slow connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f8e8d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA3.1:8B MODEL CHECK\n",
      "==================================================\n",
      "Model: llama3.1:8b\n",
      "Ollama URL: http://127.0.0.1:11435\n",
      "Model 'llama3.1:8b' is already installed.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def model_is_installed(model_name: str) -> bool:\n",
    "    \"\"\"Check if the model is already installed in Ollama\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{GENERATIVE_MODEL_URL}/api/tags\", timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return False\n",
    "        models = response.json().get(\"models\", [])\n",
    "        return any(model.get(\"name\") == model_name for model in models)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def install_model(model_name: str) -> bool:\n",
    "    \"\"\"Install the model using Ollama's API\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{GENERATIVE_MODEL_URL}/api/pull\",\n",
    "            json={\"name\": model_name},\n",
    "            timeout=600\n",
    "        )\n",
    "        return response.status_code == 200\n",
    "    except Exception as error:\n",
    "        print(f\"Error installing model: {error}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"LLAMA3.1:8B MODEL CHECK\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Ollama URL: {GENERATIVE_MODEL_URL}\")\n",
    "\n",
    "if model_is_installed(MODEL_NAME):\n",
    "    print(f\"Model '{MODEL_NAME}' is already installed.\")\n",
    "else:\n",
    "    print(f\"Model '{MODEL_NAME}' not found. Installing...\")\n",
    "    if not install_model(MODEL_NAME):\n",
    "        print(\"Model installation failed.\")\n",
    "        raise SystemExit(1)\n",
    "    print(\"Model installed successfully.\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3646795",
   "metadata": {},
   "source": [
    "## 5. Service Testing & Validation\n",
    "\n",
    "Now let's test each service individually to make sure they are working correctly. This will help you understand what each service does in our Credit OCR system.\n",
    "\n",
    "### A) PostgreSQL Database\n",
    "\n",
    "PostgreSQL stores all the structured data for our system:\n",
    "- Document metadata (filename, upload date, processing status)\n",
    "- Extracted text and data from credit documents\n",
    "- User information and processing history\n",
    "\n",
    "Let's test the database connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "rhnz43uolf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ PostgreSQL is accepting connections on port 5432\n",
      "âœ“ Successfully connected to database 'dms_meta'\n",
      "âœ“ PostgreSQL version: PostgreSQL 15.14\n",
      "\n",
      "Database connection details:\n",
      "â€¢ Host: localhost\n",
      "â€¢ Port: 5432\n",
      "â€¢ Database: dms_meta\n",
      "â€¢ Username: dms\n",
      "â€¢ Password: dms\n"
     ]
    }
   ],
   "source": [
    "# Test PostgreSQL Database Connection\n",
    "def test_postgres_connection():\n",
    "    \"\"\"Test if PostgreSQL is accessible and responding\"\"\"\n",
    "    import socket\n",
    "    \n",
    "    try:\n",
    "        # Test if PostgreSQL port is accessible\n",
    "        test_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        test_socket.settimeout(3)\n",
    "        connection_result = test_socket.connect_ex(('localhost', 5432))\n",
    "        test_socket.close()\n",
    "        \n",
    "        if connection_result == 0:\n",
    "            print(\"âœ“ PostgreSQL is accepting connections on port 5432\")\n",
    "            \n",
    "            # Try to connect with actual credentials\n",
    "            try:                \n",
    "                database_connection = psycopg2.connect(\n",
    "                    host=\"localhost\",\n",
    "                    port=5432,\n",
    "                    database=\"dms_meta\", \n",
    "                    user=\"dms\",\n",
    "                    password=\"dms\"\n",
    "                )\n",
    "                \n",
    "                print(\"âœ“ Successfully connected to database 'dms_meta'\")\n",
    "                \n",
    "                # Test simple query\n",
    "                cursor = database_connection.cursor()\n",
    "                cursor.execute(\"SELECT version();\")\n",
    "                postgres_version = cursor.fetchone()[0]\n",
    "                print(f\"âœ“ PostgreSQL version: {postgres_version.split()[0]} {postgres_version.split()[1]}\")\n",
    "                \n",
    "                cursor.close()\n",
    "                database_connection.close()\n",
    "                return True\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"âš  psycopg2 not installed - basic connection test only\")\n",
    "                return True\n",
    "            except Exception as db_error:\n",
    "                print(f\"âœ— Database connection failed: {db_error}\")\n",
    "                return False\n",
    "                \n",
    "        else:\n",
    "            print(\"âœ— PostgreSQL port 5432 is not accessible\")\n",
    "            print(\"Make sure the postgres service is running\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as connection_error:\n",
    "        print(f\"âœ— Could not test PostgreSQL connection: {connection_error}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "postgres_is_working = test_postgres_connection()\n",
    "\n",
    "if postgres_is_working:\n",
    "    print(\"\\nDatabase connection details:\")\n",
    "    print(\"â€¢ Host: localhost\")\n",
    "    print(\"â€¢ Port: 5432\") \n",
    "    print(\"â€¢ Database: dms_meta\")\n",
    "    print(\"â€¢ Username: dms\")\n",
    "    print(\"â€¢ Password: dms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337d2da",
   "metadata": {},
   "source": [
    "### B) Redis Message Broker\n",
    "\n",
    "Redis serves as the message broker for Celery background job processing in our Credit OCR system:\n",
    "- Manages document processing task queues\n",
    "- Handles job distribution across background workers\n",
    "- Tracks processing status and temporary results\n",
    "- Enables asynchronous document processing without blocking the UI\n",
    "\n",
    "Let's test Redis connectivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "my7ctahs1bi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redis is accepting connections on port 6379\n",
      "âœ“ Successfully connected to Redis\n",
      "âœ“ Redis read/write operations working\n",
      "\n",
      "Redis connection details:\n",
      "â€¢ Host: localhost\n",
      "â€¢ Port: 6379\n",
      "â€¢ Used for: Background job queue and caching\n"
     ]
    }
   ],
   "source": [
    "# Test Redis Connection\n",
    "def test_redis_connection():\n",
    "    \"\"\"Test if Redis is accessible and responding\"\"\"\n",
    "    import socket\n",
    "    \n",
    "    try:\n",
    "        # Test if Redis port is accessible\n",
    "        test_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        test_socket.settimeout(3)\n",
    "        connection_result = test_socket.connect_ex(('localhost', 6379))\n",
    "        test_socket.close()\n",
    "        \n",
    "        if connection_result == 0:\n",
    "            print(\"Redis is accepting connections on port 6379\")\n",
    "            \n",
    "            # Try to connect with redis client if available\n",
    "            try:                \n",
    "                redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "                \n",
    "                # Test ping\n",
    "                redis_response = redis_client.ping()\n",
    "                if redis_response:\n",
    "                    print(\"âœ“ Successfully connected to Redis\")\n",
    "                    \n",
    "                    # Test basic operations\n",
    "                    redis_client.set('test_key', 'test_value')\n",
    "                    retrieved_value = redis_client.get('test_key')\n",
    "                    \n",
    "                    if retrieved_value == 'test_value':\n",
    "                        print(\"âœ“ Redis read/write operations working\")\n",
    "                        redis_client.delete('test_key')  # cleanup\n",
    "                        return True\n",
    "                    else:\n",
    "                        print(\"âœ— Redis read/write test failed\")\n",
    "                        return False\n",
    "                else:\n",
    "                    print(\"âœ— Redis ping failed\")\n",
    "                    return False\n",
    "                    \n",
    "            except ImportError:\n",
    "                print(\"âš  redis-py not installed - basic connection test only\")\n",
    "                return True\n",
    "            except Exception as redis_error:\n",
    "                print(f\"âœ— Redis connection failed: {redis_error}\")\n",
    "                return False\n",
    "                \n",
    "        else:\n",
    "            print(\"Redis port 6379 is not accessible\")\n",
    "            print(\"Make sure the redis service is running\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as connection_error:\n",
    "        print(f\"Could not test Redis connection: {connection_error}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "redis_is_working = test_redis_connection()\n",
    "\n",
    "if redis_is_working:\n",
    "    print(\"\\nRedis connection details:\")\n",
    "    print(\"â€¢ Host: localhost\")\n",
    "    print(\"â€¢ Port: 6379\")\n",
    "    print(\"â€¢ Used for: Background job queue and caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olb9gxpkomk",
   "metadata": {},
   "source": [
    "### C) Ollama LLM\n",
    "\n",
    "Ollama runs LLM models locally for text analysis in our Credit OCR system:\n",
    "- Processes extracted text from documents  \n",
    "- Analyzes credit-related information\n",
    "- Runs completely on your local machine (no external API calls)\n",
    "- Privacy-focused - your data never leaves your computer\n",
    "\n",
    "**Note:** The Ollama service is mapped to port 11435 instead of the default 11434 to avoid conflicts.\n",
    "\n",
    "Let's test Ollama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rgr25nmpdx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ollama API is responding\n",
      "âœ“ Ollama version: 0.5.13\n",
      "âœ“ Found 1 installed models\n",
      "\n",
      "Installed AI models:\n",
      "  â€¢ llama3.1:8b (4.6 GB)\n",
      "\n",
      "Ollama connection details:\n",
      "â€¢ Host: localhost\n",
      "â€¢ Port: 11435\n",
      "â€¢ Used for: Local AI text analysis\n",
      "â€¢ Privacy: All processing happens locally\n"
     ]
    }
   ],
   "source": [
    "# Test Ollama LLM Model\n",
    "# Note: Using port 11435 as defined in compose.yml\n",
    "def test_ollama_service():\n",
    "    \"\"\"Test if Ollama is running and accessible\"\"\"\n",
    "    \n",
    "    # Test 1: Check if Ollama API is responding\n",
    "    try:\n",
    "        version_response = requests.get(f\"{GENERATIVE_MODEL_URL}/api/version\", timeout=10)\n",
    "        if version_response.status_code == 200:\n",
    "            version_data = version_response.json()\n",
    "            version = version_data.get('version', 'unknown')\n",
    "            print(\"âœ“ Ollama API is responding\")\n",
    "            print(f\"âœ“ Ollama version: {version}\")\n",
    "        else:\n",
    "            print(f\"âš  Ollama API returned status: {version_response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âœ— Ollama is not responding on port 11435\")\n",
    "        print(\"Make sure the ollama service is running\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"âœ— Ollama request timed out\")\n",
    "        print(\"Service might still be starting up\")\n",
    "        return False\n",
    "    except Exception as error:\n",
    "        print(f\"âœ— Error testing Ollama: {error}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2: Check available models\n",
    "    try:\n",
    "        models_response = requests.get(f\"{GENERATIVE_MODEL_URL}/api/tags\", timeout=10)\n",
    "        if models_response.status_code == 200:\n",
    "            models_data = models_response.json()\n",
    "            available_models = models_data.get('models', [])\n",
    "            \n",
    "            print(f\"âœ“ Found {len(available_models)} installed models\")\n",
    "            \n",
    "            if available_models:\n",
    "                print(\"\\nInstalled AI models:\")\n",
    "                for model in available_models:\n",
    "                    model_name = model.get('name', 'unknown')\n",
    "                    model_size = model.get('size', 0)\n",
    "                    size_in_gb = round(model_size / (1024**3), 1)\n",
    "                    print(f\"  â€¢ {model_name} ({size_in_gb} GB)\")\n",
    "            else:\n",
    "                print(\"\\nNo models installed yet\")\n",
    "                print(\"You can install models later for text analysis\")\n",
    "                print(\"Example: docker exec ollama ollama pull llama3.2:1b\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âš  Could not get model list: {models_response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as models_error:\n",
    "        print(f\"âš  Could not check models: {models_error}\")\n",
    "        return True  # Still consider success if API is working\n",
    "\n",
    "\n",
    "ollama_is_working = test_ollama_service()\n",
    "\n",
    "if ollama_is_working:\n",
    "    print(\"\\nOllama connection details:\")\n",
    "    print(\"â€¢ Host: localhost\")\n",
    "    print(\"â€¢ Port: 11435\") \n",
    "    print(\"â€¢ Used for: Local AI text analysis\")\n",
    "    print(\"â€¢ Privacy: All processing happens locally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xofrh13d2rj",
   "metadata": {},
   "source": [
    "### D) Azurite Blob Storage\n",
    "\n",
    "Azurite is a local Azure Blob Storage emulator for our Credit OCR system:\n",
    "- Stores uploaded document files (PDFs, images)\n",
    "- Provides the same API as Azure Blob Storage\n",
    "- Runs locally for development and testing\n",
    "- No external dependencies or cloud accounts needed\n",
    "\n",
    "Let's test Azurite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1w3j46f69ge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ— Azurite is not responding on port 10000\n",
      "Make sure the azurite service is running\n"
     ]
    }
   ],
   "source": [
    "# Test Azurite Blob Storage\n",
    "azurite_base_url = f\"http://{AZURITE_HOST}:{AZURITE_PORT}\"\n",
    "\n",
    "def test_azurite_service():\n",
    "    \"\"\"Test if Azurite blob storage is running and accessible\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Test if Azurite is responding to the service endpoint\n",
    "        service_response = requests.get(f\"{azurite_base_url}/devstoreaccount1\", timeout=10)\n",
    "        \n",
    "        if service_response.status_code == 400:\n",
    "            # Status 400 is expected when accessing the root - it means Azurite is running\n",
    "            print(\"âœ“ Azurite blob storage is responding\")\n",
    "            print(\"âœ“ Service is accessible on port 10000\")\n",
    "            \n",
    "            # Try to test blob service info\n",
    "            blob_service_response = requests.get(f\"{azurite_base_url}/devstoreaccount1?comp=properties&restype=service\", timeout=5)\n",
    "            if blob_service_response.status_code in [200, 400, 403]:\n",
    "                print(\"âœ“ Azurite blob service API is working\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"âš  Azurite blob service returned: {blob_service_response.status_code}\")\n",
    "                return True  # Still consider success since main service is responding\n",
    "                \n",
    "        else:\n",
    "            print(f\"âš  Azurite returned unexpected status: {service_response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âœ— Azurite is not responding on port 10000\")\n",
    "        print(\"Make sure the azurite service is running\")\n",
    "        return False\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"âœ— Azurite request timed out\")\n",
    "        print(\"Service might still be starting up\")\n",
    "        return False\n",
    "    except Exception as error:\n",
    "        print(f\"âœ— Error testing Azurite: {error}\")\n",
    "        return False\n",
    "\n",
    "azurite_is_working = test_azurite_service()\n",
    "\n",
    "if azurite_is_working:\n",
    "    print(\"\\nAzurite connection details:\")\n",
    "    print(\"â€¢ Host: localhost\")\n",
    "    print(\"â€¢ Port: 10000\")\n",
    "    print(\"â€¢ Account: devstoreaccount1 (default development account)\")\n",
    "    print(\"â€¢ Used for: Local file storage for uploaded documents\")\n",
    "    print(\"â€¢ Compatible with Azure Blob Storage APIs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4qa8r07k01v",
   "metadata": {},
   "source": [
    "## Summary of Service Tests\n",
    "\n",
    "Let's summarize the status of all services in our Credit OCR system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bh1ic7xxxrq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREDIT OCR SYSTEM - SETUP SUMMARY\n",
      "============================================================\n",
      "âœ“ PostgreSQL Database       Working\n",
      "âœ“ Redis Cache               Working\n",
      "âœ“ Ollama AI Models          Working\n",
      "âœ— Azurite Blob Storage      Not working\n",
      "------------------------------------------------------------\n",
      "âš  Some services need attention\n",
      "\n",
      "Next steps:\n",
      "1. Make sure all services are running: docker compose ps\n",
      "2. Start missing services: docker compose up -d\n",
      "3. Wait 1-2 minutes and re-run the test cells\n",
      "4. Check logs if issues persist: docker compose logs [service-name]\n"
     ]
    }
   ],
   "source": [
    "# Create Summary Report\n",
    "print(\"=\" * 60)\n",
    "print(\"CREDIT OCR SYSTEM - SETUP SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_services_working = True\n",
    "\n",
    "# Check if variables exist from previous tests\n",
    "services_status = {}\n",
    "\n",
    "if 'postgres_is_working' in locals():\n",
    "    services_status['PostgreSQL Database'] = postgres_is_working\n",
    "    all_services_working = all_services_working and postgres_is_working\n",
    "else:\n",
    "    services_status['PostgreSQL Database'] = False\n",
    "    all_services_working = False\n",
    "\n",
    "if 'redis_is_working' in locals():\n",
    "    services_status['Redis Cache'] = redis_is_working\n",
    "    all_services_working = all_services_working and redis_is_working\n",
    "else:\n",
    "    services_status['Redis Cache'] = False\n",
    "    all_services_working = False\n",
    "\n",
    "if 'ollama_is_working' in locals():\n",
    "    services_status['Ollama AI Models'] = ollama_is_working\n",
    "    all_services_working = all_services_working and ollama_is_working\n",
    "else:\n",
    "    services_status['Ollama AI Models'] = False\n",
    "    all_services_working = False\n",
    "\n",
    "if 'azurite_is_working' in locals():\n",
    "    services_status['Azurite Blob Storage'] = azurite_is_working\n",
    "    all_services_working = all_services_working and azurite_is_working\n",
    "else:\n",
    "    services_status['Azurite Blob Storage'] = False\n",
    "    all_services_working = False\n",
    "\n",
    "# Display status for each service\n",
    "for service_name, is_working in services_status.items():\n",
    "    status_symbol = \"âœ“\" if is_working else \"âœ—\"\n",
    "    status_text = \"Working\" if is_working else \"Not working\"\n",
    "    print(f\"{status_symbol} {service_name:<25} {status_text}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if all_services_working:\n",
    "    print(\"SUCCESS! All services are working correctly\")\n",
    "    print(\"\\nYour Credit OCR system is ready for:\")\n",
    "    print(\"â€¢ Document upload and storage\")\n",
    "    print(\"â€¢ Text extraction and processing\") \n",
    "    print(\"â€¢ AI-powered content analysis\")\n",
    "    print(\"â€¢ Background job processing\")\n",
    "else:\n",
    "    print(\"âš  Some services need attention\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Make sure all services are running: docker compose ps\")\n",
    "    print(\"2. Start missing services: docker compose up -d\")\n",
    "    print(\"3. Wait 1-2 minutes and re-run the test cells\")\n",
    "    print(\"4. Check logs if issues persist: docker compose logs [service-name]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f33bb",
   "metadata": {},
   "source": [
    "> This notebook has shown that our compose.yaml file is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecdf84",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qjyjq3s023m",
   "metadata": {},
   "source": [
    "## 6. Setup Complete\n",
    "\n",
    "âœ… **All services are running and tested**\n",
    "\n",
    "Your Credit OCR system infrastructure is ready. \n",
    "\n",
    "> **ðŸ“– For troubleshooting, service access details, and development guidance**, see the [README.md](./README.md)\n",
    "\n",
    "**Next:** Continue to the next notebook for document processing implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ok33sipeapa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db723wrw0x",
   "metadata": {},
   "source": [
    "### Service Access Information\n",
    "\n",
    "Once all services are running, you can access them at these locations:\n",
    "\n",
    "### Database Access\n",
    "- **Host**: localhost\n",
    "- **Port**: 5432\n",
    "- **Database**: dms_meta\n",
    "- **Username**: dms\n",
    "- **Password**: dms\n",
    "- **Recommended Tool**: DBeaver (free database client)\n",
    "\n",
    "### File Storage Access  \n",
    "- **Azurite Storage Explorer**: Available through Azure Storage Explorer\n",
    "\n",
    "### AI Model Server\n",
    "- **Ollama API**: http://localhost:11435\n",
    "- **Example**: Install models with `docker exec ollama ollama pull llama3.1:8b`\n",
    "\n",
    "### Redis Cache\n",
    "- **Host**: localhost  \n",
    "- **Port**: 6379\n",
    "- **Tool**: Redis CLI or RedisInsight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434d7cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r9r8vebw5vl",
   "metadata": {},
   "source": [
    "### Quick Commands Reference\n",
    "\n",
    "Here are the most important commands you'll use while working with your Credit OCR system:\n",
    "\n",
    "### Docker Compose Commands\n",
    "```bash\n",
    "# Start all services in background\n",
    "docker compose up -d\n",
    "\n",
    "# Check service status\n",
    "docker compose ps\n",
    "\n",
    "# View service logs\n",
    "docker compose logs [service-name]\n",
    "\n",
    "# Stop all services\n",
    "docker compose down\n",
    "\n",
    "# Restart specific service\n",
    "docker compose restart [service-name]\n",
    "```\n",
    "\n",
    "### Database Commands\n",
    "```bash\n",
    "# Connect to PostgreSQL\n",
    "docker exec -it postgres psql -U dms -d dms_meta\n",
    "\n",
    "# Backup database\n",
    "docker exec postgres pg_dump -U dms dms_meta > backup.sql\n",
    "```\n",
    "\n",
    "### Ollama AI Commands\n",
    "```bash\n",
    "# List installed models\n",
    "docker exec ollama ollama list\n",
    "\n",
    "# Install a new model\n",
    "docker exec ollama ollama pull llama3.2:1b\n",
    "\n",
    "# Remove a model\n",
    "docker exec ollama ollama rm model-name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d2587",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vv0a9iu3rk",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully completed the setup of your Credit OCR system infrastructure! \n",
    "\n",
    "**What's Running:**\n",
    "- PostgreSQL database for structured data storage\n",
    "- Redis message broker for background job processing  \n",
    "- Ollama LLM server for text analysis capabilities\n",
    "- Azurite blob storage for document file storage\n",
    "\n",
    "**Important Notes:**\n",
    "- Keep services running while developing your application\n",
    "- Services will automatically restart when you restart Docker Desktop\n",
    "- All data is stored locally on your machine\n",
    "- No external cloud services or API keys required\n",
    "\n",
    "**You're Ready For:**\n",
    "- Document upload and storage\n",
    "- OCR text extraction implementation  \n",
    "- AI-powered document analysis\n",
    "- Building REST APIs for your application\n",
    "\n",
    "Continue to the next notebook to start building your Credit OCR application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
