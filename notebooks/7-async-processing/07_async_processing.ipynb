{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Async Document Processing with Celery\n",
        "\n",
        "*Process documents asynchronously using Celery workers for scalable, non-blocking document processing*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What you will do\n",
        "\n",
        "By the end of this notebook you will:\n",
        "- Upload a document to the DMS\n",
        "- Trigger async processing using Celery\n",
        "- Monitor processing status in real-time\n",
        "- See how async processing enables scalable document handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prerequisites\n",
        "\n",
        "- Completed notebooks 1â€“6\n",
        "- Docker services running (`docker compose up -d`)\n",
        "- Celery worker running (`docker compose up celery-worker`)\n",
        "- `data/loan_application.pdf` available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment configuration\n",
        "\n",
        "### 2.1 Imports and project root detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# Detect project root dynamically\n",
        "current_file = Path.cwd()\n",
        "project_root_directory = None\n",
        "\n",
        "# Look for project root by finding pyproject.toml\n",
        "for parent in current_file.parents:\n",
        "    if (parent / \"pyproject.toml\").exists():\n",
        "        project_root_directory = parent\n",
        "        break\n",
        "\n",
        "if project_root_directory is None:\n",
        "    raise RuntimeError(\"Could not find project root directory\")\n",
        "\n",
        "# Add project root to Python path for imports\n",
        "sys.path.insert(0, str(project_root_directory))\n",
        "\n",
        "# Change to project root for relative paths\n",
        "os.chdir(project_root_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Service clients\n",
        "\n",
        "We connect to the running services and initialize the async processor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment ready: connected to Postgres, Azurite, and Celery\n"
          ]
        }
      ],
      "source": [
        "import psycopg2\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import importlib\n",
        "\n",
        "# Import DMS and async processing modules\n",
        "from src.dms.service import DmsService\n",
        "from src.dms.adapters import AzureBlobStorageClient, PostgresMetadataRepository\n",
        "from src.async_processing import AsyncDocumentProcessor\n",
        "\n",
        "# Reload modules to ensure latest code\n",
        "import src.dms.adapters as dms_adapters\n",
        "importlib.reload(dms_adapters)\n",
        "\n",
        "# Config for compose-based services\n",
        "POSTGRES_HOST: str = \"localhost\"\n",
        "POSTGRES_PORT: int = 5432\n",
        "POSTGRES_DBNAME: str = \"dms_meta\"\n",
        "POSTGRES_USER: str = \"dms\"\n",
        "POSTGRES_PASSWORD: str = \"dms\"\n",
        "\n",
        "AZURITE_BLOB_PORT: int = 10000\n",
        "AZURITE_ACCOUNT_NAME: str = \"devstoreaccount1\"\n",
        "AZURITE_ACCOUNT_KEY: str = (\n",
        "    \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/\"\n",
        "    \"K1SZFPTOtr/KBHBeksoGMGw==\"\n",
        ")\n",
        "CONTAINER_NAME: str = \"documents\"\n",
        "\n",
        "# Initialize clients\n",
        "connection_string = (\n",
        "    \"DefaultEndpointsProtocol=http;\"\n",
        "    f\"AccountName={AZURITE_ACCOUNT_NAME};\"\n",
        "    f\"AccountKey={AZURITE_ACCOUNT_KEY};\"\n",
        "    f\"BlobEndpoint=http://localhost:{AZURITE_BLOB_PORT}/devstoreaccount1;\"\n",
        ")\n",
        "\n",
        "blob_service_client: BlobServiceClient = BlobServiceClient.from_connection_string(connection_string)\n",
        "container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "pg_conn = psycopg2.connect(\n",
        "    host=POSTGRES_HOST,\n",
        "    port=POSTGRES_PORT,\n",
        "    database=POSTGRES_DBNAME,\n",
        "    user=POSTGRES_USER,\n",
        "    password=POSTGRES_PASSWORD,\n",
        ")\n",
        "\n",
        "storage_client = AzureBlobStorageClient(blob_service_client)\n",
        "metadata_repo = PostgresMetadataRepository(pg_conn)\n",
        "dms_service = DmsService(storage_client=storage_client, metadata_repository=metadata_repo)\n",
        "\n",
        "# Initialize async processor\n",
        "async_processor = AsyncDocumentProcessor()\n",
        "\n",
        "print(\"Environment ready: connected to Postgres, Azurite, and Celery\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Database schema\n",
        "\n",
        "Apply the schema to ensure all tables exist:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database schema applied successfully\n",
            "Tables ensured:\n",
            "- documents (text_extraction_status, processing_status)\n",
            "- extraction_jobs\n",
            "- ocr_results\n"
          ]
        }
      ],
      "source": [
        "# Apply database schema\n",
        "schema_path = project_root_directory / \"database\" / \"schemas\" / \"schema.sql\"\n",
        "schema_sql = schema_path.read_text()\n",
        "\n",
        "with pg_conn.cursor() as cursor:\n",
        "    cursor.execute(schema_sql)\n",
        "    pg_conn.commit()\n",
        "\n",
        "print(\"Database schema applied successfully\")\n",
        "print(\"Tables ensured:\")\n",
        "print(\"- documents (text_extraction_status, processing_status)\")\n",
        "print(\"- extraction_jobs\")\n",
        "print(\"- ocr_results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload document\n",
        "\n",
        "Upload a document to the DMS for async processing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document uploaded with ID: 6b08be4b-8f81-48db-934e-552306074161\n",
            "Initial text extraction status: ready\n",
            "Initial processing status: pending extraction\n"
          ]
        }
      ],
      "source": [
        "# Upload document\n",
        "test_file_path = project_root_directory / \"data\" / \"loan_application.pdf\"\n",
        "\n",
        "if not test_file_path.exists():\n",
        "    print(f\"Test file not found: {test_file_path}\")\n",
        "else:\n",
        "    document_id = dms_service.upload_document(\n",
        "        file_path=test_file_path,\n",
        "        document_type=\"loan_application\",\n",
        "        source_filename=\"loan_application.pdf\",\n",
        "    )\n",
        "    print(f\"Document uploaded with ID: {document_id}\")\n",
        "\n",
        "    # Check initial status\n",
        "    document = dms_service.get_document(document_id)\n",
        "    print(f\"Initial text extraction status: {document.get('textextraction_status')}\")\n",
        "    print(f\"Initial processing status: {document.get('processing_status')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Trigger async processing\n",
        "\n",
        "Start the async processing pipeline:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Triggering async processing...\n",
            "Async processing started with task ID: a8b29ece-3446-4071-b3ed-4cf2e1607f00\n",
            "Processing is now running in the background Celery worker\n"
          ]
        }
      ],
      "source": [
        "# Trigger async processing\n",
        "print(\"Triggering async processing...\")\n",
        "task_id = async_processor.trigger_processing(document_id)\n",
        "\n",
        "if task_id:\n",
        "    print(f\"Async processing started with task ID: {task_id}\")\n",
        "    print(\"Processing is now running in the background Celery worker\")\n",
        "else:\n",
        "    print(\"Failed to start async processing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Monitor processing status\n",
        "\n",
        "Monitor the processing status in real-time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monitoring processing status...\n",
            "This may take a few minutes as the Celery worker processes the document\n",
            "\n",
            "--- Status Check 1 ---\n",
            "Text extraction status: ready\n",
            "Processing status: pending extraction\n",
            "Latest job status: pending extraction\n",
            "\n",
            "--- Status Check 2 ---\n",
            "Text extraction status: ready\n",
            "Processing status: ocr running\n",
            "Latest job status: pending extraction\n",
            "\n",
            "--- Status Check 3 ---\n",
            "Text extraction status: ready\n",
            "Processing status: ocr running\n",
            "Latest job status: pending extraction\n",
            "\n",
            "--- Status Check 4 ---\n",
            "Text extraction status: ready\n",
            "Processing status: ocr running\n",
            "Latest job status: pending extraction\n",
            "\n",
            "--- Status Check 5 ---\n",
            "Text extraction status: ready\n",
            "Processing status: llm running\n",
            "Latest job status: pending extraction\n",
            "\n",
            "--- Status Check 6 ---\n",
            "Text extraction status: ready\n",
            "Processing status: llm running\n",
            "Latest job status: pending extraction\n",
            "\n",
            "--- Status Check 7 ---\n",
            "Text extraction status: ready\n",
            "Processing status: llm running\n",
            "Latest job status: pending extraction\n",
            "\n",
            "--- Status Check 8 ---\n",
            "Text extraction status: ready\n",
            "Processing status: llm running\n",
            "Latest job status: pending extraction\n",
            "\n",
            "--- Status Check 9 ---\n",
            "Text extraction status: ready\n",
            "Processing status: done\n",
            "Latest job status: pending extraction\n",
            "\n",
            "âœ“ Processing completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Monitor processing status\n",
        "print(\"Monitoring processing status...\")\n",
        "print(\"This may take a few minutes as the Celery worker processes the document\")\n",
        "\n",
        "for i in range(20):  # Monitor for up to 20 iterations\n",
        "    status = async_processor.get_processing_status(document_id)\n",
        "    \n",
        "    print(f\"\\n--- Status Check {i+1} ---\")\n",
        "    print(f\"Text extraction status: {status.get('text_extraction_status')}\")\n",
        "    print(f\"Processing status: {status.get('processing_status')}\")\n",
        "    \n",
        "    # Check extraction jobs\n",
        "    jobs = status.get('extraction_jobs', [])\n",
        "    if jobs:\n",
        "        latest_job = jobs[0]\n",
        "        print(f\"Latest job status: {latest_job.get('status')}\")\n",
        "        if latest_job.get('error_message'):\n",
        "            print(f\"Error: {latest_job.get('error_message')}\")\n",
        "    \n",
        "    # Check if processing is complete\n",
        "    if status.get('processing_status') == 'done':\n",
        "        print(\"\\nâœ“ Processing completed successfully!\")\n",
        "        break\n",
        "    elif status.get('processing_status') == 'failed':\n",
        "        print(\"\\nâœ— Processing failed\")\n",
        "        break\n",
        "    \n",
        "    # Wait before next check\n",
        "    time.sleep(7)\n",
        "else:\n",
        "    print(\"\\nMonitoring timeout - processing may still be running\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cleanup\n",
        "\n",
        "Close database connections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database connection closed\n"
          ]
        }
      ],
      "source": [
        "pg_conn.close()\n",
        "print(\"Database connection closed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- **Async Processing**: Documents are processed in the background using Celery workers\n",
        "- **Scalability**: Multiple workers can process documents concurrently\n",
        "- **Status Tracking**: Real-time monitoring of processing status and job progress\n",
        "- **Error Handling**: Failed tasks are properly logged and status is updated\n",
        "- **Non-blocking**: The main application doesn't wait for processing to complete\n",
        "\n",
        "For a deeper dive (theory, benefits, scaling patterns), see the README in this folder.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
