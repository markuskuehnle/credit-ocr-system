{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Document Processing Status & Extraction Jobs\n",
        "\n",
        "*Track document readiness, pipeline progress, and job lifecycle with clear statuses*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What you will do\n",
        "\n",
        "By the end of this notebook you will:\n",
        "- Upload a document and observe initial statuses\n",
        "- See when and how extraction jobs are created\n",
        "- Drive status transitions through a simulated pipeline (OCR → LLM → Done)\n",
        "- Inspect failure paths and error recording\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prerequisites\n",
        "\n",
        "- Completed notebooks 1–5\n",
        "- Docker services running (`docker compose up -d`)\n",
        "- `data/loan_application.pdf` available\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment configuration\n",
        "\n",
        "### 2.1 Imports and project root detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/markuskuehnle/Documents/projects/credit-ocr-system\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import uuid\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Detect project root dynamically\n",
        "current_file = Path.cwd()\n",
        "project_root_directory = None\n",
        "\n",
        "# Look for project root by finding pyproject.toml\n",
        "for parent in current_file.parents:\n",
        "    if (parent / \"pyproject.toml\").exists():\n",
        "        project_root_directory = parent\n",
        "        break\n",
        "\n",
        "if project_root_directory is None:\n",
        "    raise RuntimeError(\"Could not find project root directory\")\n",
        "\n",
        "print(f\"Project root: {project_root_directory}\")\n",
        "\n",
        "# Add project root to Python path for imports\n",
        "sys.path.insert(0, str(project_root_directory))\n",
        "\n",
        "# Change to project root for relative paths\n",
        "os.chdir(project_root_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Service clients\n",
        "\n",
        "We connect to the running Postgres and Azurite services from `compose.yml` and construct the DMS service with adapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment ready: connected to Postgres and Azurite\n"
          ]
        }
      ],
      "source": [
        "import psycopg2\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import importlib\n",
        "\n",
        "# Import DMS modules\n",
        "from src.dms.service import DmsService\n",
        "from src.dms.adapters import AzureBlobStorageClient, PostgresMetadataRepository\n",
        "\n",
        "# Reload modules to ensure latest code\n",
        "import src.dms.adapters as dms_adapters\n",
        "importlib.reload(dms_adapters)\n",
        "\n",
        "# Config for compose-based services\n",
        "POSTGRES_HOST: str = \"localhost\"\n",
        "POSTGRES_PORT: int = 5432\n",
        "POSTGRES_DBNAME: str = \"dms_meta\"\n",
        "POSTGRES_USER: str = \"dms\"\n",
        "POSTGRES_PASSWORD: str = \"dms\"\n",
        "\n",
        "AZURITE_BLOB_PORT: int = 10000\n",
        "AZURITE_ACCOUNT_NAME: str = \"devstoreaccount1\"\n",
        "AZURITE_ACCOUNT_KEY: str = (\n",
        "    \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/\"\n",
        "    \"K1SZFPTOtr/KBHBeksoGMGw==\"\n",
        ")\n",
        "CONTAINER_NAME: str = \"documents\"\n",
        "\n",
        "# Optionally reuse an existing connection string if set\n",
        "existing_conn_str: str | None = os.environ.get(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
        "if existing_conn_str:\n",
        "    connection_string: str = existing_conn_str\n",
        "else:\n",
        "    connection_string = (\n",
        "        \"DefaultEndpointsProtocol=http;\"\n",
        "        f\"AccountName={AZURITE_ACCOUNT_NAME};\"\n",
        "        f\"AccountKey={AZURITE_ACCOUNT_KEY};\"\n",
        "        f\"BlobEndpoint=http://localhost:{AZURITE_BLOB_PORT}/devstoreaccount1;\"\n",
        "    )\n",
        "\n",
        "# Initialize clients\n",
        "blob_service_client: BlobServiceClient = BlobServiceClient.from_connection_string(connection_string)\n",
        "container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
        "try:\n",
        "    container_client.create_container()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "pg_conn = psycopg2.connect(\n",
        "    host=POSTGRES_HOST,\n",
        "    port=POSTGRES_PORT,\n",
        "    database=POSTGRES_DBNAME,\n",
        "    user=POSTGRES_USER,\n",
        "    password=POSTGRES_PASSWORD,\n",
        ")\n",
        "\n",
        "storage_client = AzureBlobStorageClient(blob_service_client)\n",
        "metadata_repo = PostgresMetadataRepository(pg_conn)\n",
        "\n",
        "dms_service = DmsService(storage_client=storage_client, metadata_repository=metadata_repo)\n",
        "\n",
        "print(\"Environment ready: connected to Postgres and Azurite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DMS service recreated with updated adapter\n"
          ]
        }
      ],
      "source": [
        "# Reload adapter module to pick up latest changes\n",
        "importlib.reload(dms_adapters)\n",
        "from src.dms.adapters import AzureBlobStorageClient, PostgresMetadataRepository\n",
        "\n",
        "# Recreate the service with updated adapter\n",
        "storage_client = AzureBlobStorageClient(blob_service_client)\n",
        "metadata_repo = PostgresMetadataRepository(pg_conn)\n",
        "dms_service = DmsService(storage_client=storage_client, metadata_repository=metadata_repo)\n",
        "\n",
        "print(\"DMS service recreated with updated adapter\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Database schema\n",
        "\n",
        "Apply the schema to ensure status columns and extraction jobs table exist:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database schema applied successfully\n",
            "Tables ensured:\n",
            "- documents (text_extraction_status, processing_status)\n",
            "- extraction_jobs\n",
            "- ocr_results\n"
          ]
        }
      ],
      "source": [
        "# Apply database schema\n",
        "schema_path = project_root_directory / \"database\" / \"schemas\" / \"schema.sql\"\n",
        "schema_sql = schema_path.read_text()\n",
        "\n",
        "with pg_conn.cursor() as cursor:\n",
        "    cursor.execute(schema_sql)\n",
        "    pg_conn.commit()\n",
        "\n",
        "print(\"Database schema applied successfully\")\n",
        "print(\"Tables ensured:\")\n",
        "print(\"- documents (text_extraction_status, processing_status)\")\n",
        "print(\"- extraction_jobs\")\n",
        "print(\"- ocr_results\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload workflow\n",
        "\n",
        "We will upload a document and observe the initial statuses and created extraction jobs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document uploaded with ID: 3348fafb-1452-476c-9a11-368088517fa8\n",
            "\n",
            "Initial document status:\n",
            "- Text extraction status: ready\n",
            "- Processing status: pending extraction\n",
            "\n",
            "Extraction jobs created: 1\n",
            "- Job ID: f5542c58-62c6-410d-9060-441992686f15\n",
            "  Status: pending extraction\n",
            "  Created: 2025-09-17 20:50:11.176235+00:00\n"
          ]
        }
      ],
      "source": [
        "# Upload a test document and inspect initial statuses\n",
        "\n",
        "test_file_path = project_root_directory / \"data\" / \"loan_application.pdf\"\n",
        "\n",
        "if not test_file_path.exists():\n",
        "    print(f\"Test file not found: {test_file_path}\")\n",
        "else:\n",
        "    document_id = dms_service.upload_document(\n",
        "        file_path=test_file_path,\n",
        "        document_type=\"loan_application\",\n",
        "        source_filename=\"loan_application.pdf\",\n",
        "    )\n",
        "    print(f\"Document uploaded with ID: {document_id}\")\n",
        "    \n",
        "    # Retrieve document record\n",
        "    document = dms_service.get_document(document_id)\n",
        "    print(\"\\nInitial document status:\")\n",
        "    print(f\"- Text extraction status: {document.get('textextraction_status', 'N/A')}\")\n",
        "    print(f\"- Processing status: {document.get('processing_status', 'N/A')}\")\n",
        "    \n",
        "    # List jobs\n",
        "    jobs = dms_service.get_extraction_jobs(document_id)\n",
        "    print(f\"\\nExtraction jobs created: {len(jobs)}\")\n",
        "    for job in jobs:\n",
        "        print(f\"- Job ID: {job['id']}\")\n",
        "        print(f\"  Status: {job['status']}\")\n",
        "        print(f\"  Created: {job['created_at']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Simulate pipeline status updates\n",
        "\n",
        "We simulate OCR and LLM steps, updating both status models accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Simulating processing ===\n",
            "\n",
            "1. Marking 'ready'...\n",
            "   ✓ Text extraction status → 'ready'\n",
            "\n",
            "2. OCR running...\n",
            "   ✓ Processing status → 'ocr running'\n",
            "\n",
            "3. LLM running...\n",
            "   ✓ Processing status → 'llm running'\n",
            "\n",
            "4. Completing processing...\n",
            "   ✓ Text extraction status → 'completed'\n",
            "   ✓ Processing status → 'done'\n",
            "   ✓ Extraction job f5542c58-62c6-410d-9060-441992686f15 → 'done'\n"
          ]
        }
      ],
      "source": [
        "# Simulate processing status transitions\n",
        "print(\"=== Simulating processing ===\\n\")\n",
        "\n",
        "# Mark ready\n",
        "print(\"1. Marking 'ready'...\")\n",
        "dms_service.update_textextraction_status(document_id, \"ready\")\n",
        "print(\"   ✓ Text extraction status → 'ready'\")\n",
        "\n",
        "# OCR running\n",
        "print(\"\\n2. OCR running...\")\n",
        "dms_service.mark_ocr_running(document_id)\n",
        "print(\"   ✓ Processing status → 'ocr running'\")\n",
        "\n",
        "# LLM running\n",
        "print(\"\\n3. LLM running...\")\n",
        "dms_service.mark_llm_running(document_id)\n",
        "print(\"   ✓ Processing status → 'llm running'\")\n",
        "\n",
        "# Finish\n",
        "print(\"\\n4. Completing processing...\")\n",
        "dms_service.update_textextraction_status(document_id, \"completed\")\n",
        "dms_service.mark_processing_done(document_id)\n",
        "print(\"   ✓ Text extraction status → 'completed'\")\n",
        "print(\"   ✓ Processing status → 'done'\")\n",
        "\n",
        "# Mark job done (first job)\n",
        "jobs = dms_service.get_extraction_jobs(document_id)\n",
        "if jobs:\n",
        "    job_id = jobs[0]['id']\n",
        "    dms_service.update_extraction_job(job_id, \"done\")\n",
        "    print(f\"   ✓ Extraction job {job_id} → 'done'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Verify results\n",
        "\n",
        "Check the final document and job statuses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Final Document ===\n",
            "ID: 3348fafb-1452-476c-9a11-368088517fa8\n",
            "Filename: loan_application.pdf\n",
            "Text extraction status: completed\n",
            "Processing status: done\n",
            "\n",
            "=== Extraction Jobs ===\n",
            "Job ID: f5542c58-62c6-410d-9060-441992686f15\n",
            "Status: done\n",
            "Created: 2025-09-17 20:50:11.176235+00:00\n",
            "Completed: 2025-09-17 20:50:11.184303+00:00\n",
            "Error: None\n"
          ]
        }
      ],
      "source": [
        "# Final document status\n",
        "document = dms_service.get_document(document_id)\n",
        "print(\"=== Final Document ===\")\n",
        "print(f\"ID: {document_id}\")\n",
        "print(f\"Filename: {document.get('source_filename')}\")\n",
        "print(f\"Text extraction status: {document.get('textextraction_status', 'N/A')}\")\n",
        "print(f\"Processing status: {document.get('processing_status', 'N/A')}\")\n",
        "\n",
        "# Final job status\n",
        "jobs = dms_service.get_extraction_jobs(document_id)\n",
        "print(\"\\n=== Extraction Jobs ===\")\n",
        "for job in jobs:\n",
        "    print(f\"Job ID: {job['id']}\")\n",
        "    print(f\"Status: {job['status']}\")\n",
        "    print(f\"Created: {job['created_at']}\")\n",
        "    print(f\"Completed: {job.get('completed_at', 'N/A')}\")\n",
        "    print(f\"Error: {job.get('error_message', 'None')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cleanup\n",
        "\n",
        "Close database connections and release resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database connection closed\n"
          ]
        }
      ],
      "source": [
        "pg_conn.close()\n",
        "print(\"Database connection closed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Implemented and demonstrated two status models: text extraction status and processing status\n",
        "- Showed automatic extraction job creation and lifecycle updates\n",
        "- Simulated pipeline-driven status updates (OCR → LLM → Done)\n",
        "- Highlighted failure handling and error recording\n",
        "\n",
        "For a deeper dive (theory, tradeoffs, best practices), see the README in this folder.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
